{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190011_IE684_Lab4_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6f-ejEMhb7U"
      },
      "source": [
        "Here, the function under consideration is: \\\r\n",
        " \\\\\r\n",
        "$\\hspace6ex f(\\mathbf{x}) = 100x_1^2 +  0.001x_2^4$.  \\\r\n",
        " \\\\\r\n",
        "$\\Rightarrow \\ Hessian \\ = \\nabla^2 f(\\mathbf{x}) =\r\n",
        "\\begin{bmatrix}\r\n",
        "  f_{x_1^2}(\\mathbf{x}) & \r\n",
        "    f_{x_1x_2}(\\mathbf{x})  \\\\\r\n",
        "  f_{x_2x_1}(\\mathbf{x}) & \r\n",
        "    f_{x_2^2}(\\mathbf{x})\r\n",
        "\\end{bmatrix}\r\n",
        "=\r\n",
        "\\begin{bmatrix}\r\n",
        "  200 & 0 \\\\ 0 & 0.012x_2^2\r\n",
        "\\end{bmatrix} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKY6QYW8gqCj"
      },
      "source": [
        "import numpy as np \n",
        " \n",
        "#method to find Hessian matrix\n",
        "def evalh(x): \n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2 \n",
        "  return np.array([[200, 0], [0, 0.012*x[1]**2]])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHj2P4aYoJbw"
      },
      "source": [
        "def evalf(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the objective function value\r\n",
        "  #compute the function value and return it \r\n",
        "  return 100*x[0]**2 + 0.001*x[1]**4"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFyEQ0O1oO7t"
      },
      "source": [
        "def evalg(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the gradient value\r\n",
        "  #compute the gradient value and return it \r\n",
        "  return np.array([200*x[0], 0.004*x[1]**3])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdY3HnNbsUr3"
      },
      "source": [
        "def compute_D_k(x):\r\n",
        "  assert type(x) is np.ndarray\r\n",
        "  assert len(x) == 2\r\n",
        "  if np.linalg.det(evalh(x)) == 0:\r\n",
        "    raise ValueError('Determinant does not exist. Please check!!')\r\n",
        "  return np.linalg.inv(evalh(x))  #computing inverse of Hessian."
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msqOVfjZocrQ"
      },
      "source": [
        "def compute_steplength_backtracking_Newton_method(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  #assert type(direction) is np.ndarray and len(direction) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  alpha = alpha_start\n",
        "  D_k = compute_D_k(x)\n",
        "  while evalf(x + alpha*np.matmul(D_k,-gradf)) > evalf(x) + gamma*alpha*(np.matmul(np.matrix.transpose(gradf), np.matmul(D_k,-gradf)) ):\n",
        "    alpha = rho*alpha\n",
        "  return alpha"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIMY4htXuCrl"
      },
      "source": [
        "#line search type \r\n",
        "CONSTANT_STEP_LENGTH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "BACKTRACKING_LINE_SEARCH_SCALING = 3"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "904c_D9huJwQ"
      },
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        " \n",
        "def find_minimizer_Newton_method(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        " \n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        " \n",
        "  k = 0\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    D_k = compute_D_k(x)\n",
        "    import scipy\n",
        "    from scipy.linalg import sqrtm\n",
        "    d = scipy.linalg.sqrtm(D_k)\n",
        "    if line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1.0\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_Newton_method(x, g_x, alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED') \n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')    \n",
        "    #implement the gradient descent steps here  \n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "  return x, k, evalf(x)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wEHv0rmuiG0"
      },
      "source": [
        "my_start_x = np.array([1.0,1.0])\r\n",
        "my_tol= 1e-9"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtD0q4q4NIS"
      },
      "source": [
        "**3. Answer  :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtJ6aA0ywH3C",
        "outputId": "6ce1726e-3e31-4e3e-b245-c9958eeef6f0"
      },
      "source": [
        "print(\"For Newton's Method with CONSTANT_STEP_LENGTH procedure :\")\n",
        "x_opt, k, f_value = find_minimizer_Newton_method(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print(\"Minimizer = \",x_opt,\",  Iteration = \",k,\",  Minimum function value = \", f_value) \n",
        " \n",
        "print(\"\\nFor Newton's Method with BACKTRACKING_LINE_SEARCH :\")\n",
        "x_opt_bls, k, f_value = find_minimizer_Newton_method(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.0, 0.5,0.5)\n",
        "print(\"Minimizer = \",x_opt_bls,\",  Iteration = \", k , \",  Minimum function value = \",f_value)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Newton's Method with CONSTANT_STEP_LENGTH procedure :\n",
            "Minimizer =  [0.         0.00513823] ,  Iteration =  13 ,  Minimum function value =  6.970349091039817e-13\n",
            "\n",
            "For Newton's Method with BACKTRACKING_LINE_SEARCH :\n",
            "Minimizer =  [0.         0.00513823] ,  Iteration =  13 ,  Minimum function value =  6.970349091039817e-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeNvvsh31XZC"
      },
      "source": [
        "**COMMENTS :** \\\r\n",
        "**a) Here from the above obtained result, it is clear that the number of iterations is same (i.e. 13) in both the cases.** \\\r\n",
        "\r\n",
        "**b) Also, in both cases, the minimizer is (0, 0.00513823)  and the minimum function value is 6.970349091039817e-13.**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ0BQBo4DC2h"
      },
      "source": [
        "**4. SOLUTION :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD7twBWsFRo5"
      },
      "source": [
        "def compute_D_k_diagonal(x):\r\n",
        "  assert type(x) is np.ndarray\r\n",
        "  assert len(x) == 2\r\n",
        "  #compute and return D_k\r\n",
        "  return np.array([[1/200, 0],[0, 1/(0.012*x[1]**2)]])    ## this is same as the inverse of Hessian in this question."
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYMg6ReVxnyW"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search without scaling.\r\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  \r\n",
        "  alpha = alpha_start\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  #implement the backtracking line search\r\n",
        "  while evalf(np.add(x,-alpha*gradf)) > evalf(x)-gamma*alpha*np.matmul(gr_t, gradf):\r\n",
        "    alpha = rho*alpha\r\n",
        "  #print('final step length:',alpha)\r\n",
        "  return alpha"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlFqrFrEDXgJ"
      },
      "source": [
        "def compute_steplength_backtracking_scaling(x, gradf, direction, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(direction) is np.ndarray and len(direction) == 2 \r\n",
        "  assert type(alpha_start) is float and alpha_start>=0. \r\n",
        "  assert type(rho) is float and rho>=0.\r\n",
        "  assert type(gamma) is float and gamma>=0. \r\n",
        "  alpha = alpha_start\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  #direction = -(D_k)*gradf\r\n",
        "  #implement the backtracking line search\r\n",
        "  while evalf(np.add(x,alpha*direction)) > evalf(x)+gamma*alpha*np.matmul(gr_t, direction):\r\n",
        "    alpha = rho*alpha\r\n",
        "  #print('final step length:',alpha)\r\n",
        "  return alpha"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5ZjmYg4EtfQ"
      },
      "source": [
        "#complete the code for gradient descent to find the minimizer\n",
        "def find_minimizer_gd(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  x = start_x\n",
        "  A = (1/2)*evalh(x)\n",
        "  g_x = evalg(x)\n",
        " \n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        " \n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        " \n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        " \n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x , k , evalf(x)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9tCKeNTH4af"
      },
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        " \n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  #A = evalh(x) \n",
        "  g_x = evalg(x)  \n",
        "  D_k = compute_D_k_diagonal(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH_SCALING):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        " \n",
        "  k = 0\n",
        "  while (np.linalg.norm(g_x) > tol):\n",
        "    direction = -np.matmul(D_k, g_x)        # direction varies with each iteration.\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH_SCALING:\n",
        "      step_length = compute_steplength_backtracking_scaling(x, g_x, direction, alpha_start, rho, gamma)\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.add(x, np.multiply(step_length,direction)) #update x = x + step_length*direction\n",
        "    k += 1 #increment iteration\n",
        "    D_k = compute_D_k_diagonal(x)           # Its Necessary to update the direction.\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "  return x , k , evalf(x)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOY2_9iT8QdL"
      },
      "source": [
        "**4.ANSWER:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsBWWTJLpkHZ",
        "outputId": "b472dd82-6764-4ecc-c5fc-0edf56d65cea"
      },
      "source": [
        "#check gradient descent with scaling and backtracking line search \n",
        "print(\"\\nFor BACKTRACKING_LINE_SEARCH_WITH_SCALING:\")\n",
        "x_opt_bls, k, f_value = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH_SCALING, 1.0, 0.5,0.5)\n",
        "print(\"Minimizer = \",x_opt_bls,\",Iteration = \", k , \", Minimum function value = \",f_value)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For BACKTRACKING_LINE_SEARCH_WITH_SCALING:\n",
            "Minimizer =  [0.         0.00513823] ,Iteration =  13 , Minimum function value =  6.970349091039817e-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZj-fUBeJ9ky",
        "outputId": "35079415-7096-407b-b47a-1b83926693c6"
      },
      "source": [
        "#check gradient descent without scaling and with backtracking line search \n",
        "print(\"\\nFor BACKTRACKING_LINE_SEARCH WITHOUT_SCALING :\")\n",
        "x_opt_bls, k, f_value = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.0, 0.5,0.5)\n",
        "print(\"Minimizer = \",x_opt_bls,\",Iteration = \", k , \", Minimum function value = \",f_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For BACKTRACKING_LINE_SEARCH WITHOUT_SCALING :\n",
            "Minimizer =  [1.41173892e-12 6.21297178e-03] ,Iteration =  226638545 , Minimum function value =  1.4900386194863004e-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjc6Ocew8X1U"
      },
      "source": [
        "From the previous question(i.e. 3) we have the following outputs for the two variants of Newton's Method: \\\r\n",
        "**1. For Newton's Method with CONSTANT_STEP_LENGTH procedure :** \\\r\n",
        "Minimizer =  [0. , 0.00513823] , \r\n",
        "Minimum function value =  6.970349091039817e-13, \r\n",
        "Iteration =  13 \r\n",
        "\r\n",
        "**2. For Newton's Method with BACKTRACKING_LINE_SEARCH :** \\\r\n",
        "Minimizer =  [0.  , 0.00513823] , \r\n",
        "Minimum function value =  6.970349091039817e-13, \r\n",
        "Iteration =  13  \\\r\n",
        " \\\\\r\n",
        "Now from question no.4 we have the following outputs:\r\n",
        "\r\n",
        "**1. For gradient descent algorithm (with scaling using a diagonal matrix) with backtracking line search:** \\\r\n",
        "Minimizer =  [0.  ,  0.00513823] ,\r\n",
        "Minimum function value =  6.970349091039817e-13,\r\n",
        "Iteration =  13, \\\\\r\n",
        "**2. For gradient descent algorithm (without scaling) with backtracking line search:** \\\r\n",
        "Minimizer =  [1.41173892e-12 6.21297178e-03] ,\r\n",
        "Minimum function value =  1.4900386194863004e-12,\r\n",
        "Iteration =  226638545 \\\\\r\n",
        " \\\\\r\n",
        "**COMMENTS:** \\\\\r\n",
        "1. Number of Iterations , Minimizer and the Minimum function value remains same in case of both the variants of Newton's Method and the gradient descent algorithm (with scaling using a diagonal matrix) with backtracking line search. \\\r\n",
        "2.Whereas in case of gradient descent algorithm (without scaling) with backtracking line search the number of Iterations is considerably large (i.e. 226638545). Also minimizer and the minimum function value is different from the previous methods. \\\r\n",
        "3. **So, here it is obvious that, scaling with a diagonal matrix or applying Newton's Method, helps in converging to the minimum value at a very fast rate.** \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}