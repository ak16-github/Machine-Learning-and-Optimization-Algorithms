{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190011_IE684_Lab4_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3bRupbQQ7is"
      },
      "source": [
        "Here the Given function is: \\\r\n",
        "$\\hspace6ex q(\\mathbf{x}) = \\sqrt{x_1^2 + 1} +  \\sqrt{x_2^2 + 1}$. \\\r\n",
        "\r\n",
        "$\\Rightarrow \\ Hessian \\ = \\nabla^2 q(\\mathbf{x}) =\r\n",
        "\\begin{bmatrix}\r\n",
        "  q_{x_1^2}(\\mathbf{x}) & \r\n",
        "    q_{x_1x_2}(\\mathbf{x})  \\\\\r\n",
        "  q_{x_2x_1}(\\mathbf{x}) & \r\n",
        "    q_{x_2^2}(\\mathbf{x})\r\n",
        "\\end{bmatrix}\r\n",
        "=\r\n",
        "\\begin{bmatrix}\r\n",
        "  \\frac{1}{(x_1^2+1)^\\frac{3}{2}} & 0 \\\\ \r\n",
        "  \\\\\r\n",
        "  0 & \\frac{1}{(x_2^2+1)^\\frac{3}{2}}\r\n",
        "\\end{bmatrix} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E7FTvKOLfnc"
      },
      "source": [
        "import numpy as np \r\n",
        "\r\n",
        "#method to find Hessian matrix\r\n",
        "def evalh(x): \r\n",
        "  assert type(x) is np.ndarray \r\n",
        "  assert len(x) == 2 \r\n",
        "  return np.array([[(x[0]**2+1)**(-1.5), 0], [0, (x[1]**2+1)**(-1.5)]])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yaaqWXmmliQ"
      },
      "source": [
        "def evalf(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the objective function value\r\n",
        "  #compute the function value and return it \r\n",
        "  return (x[0]**2+1)**0.5 + (x[1]**2+1)**0.5"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02v6QkUSmrud"
      },
      "source": [
        "def evalg(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the gradient value\r\n",
        "  #compute the gradient value and return it \r\n",
        "  return np.array([x[0]/(x[0]**2+1)**0.5, x[1]/(x[1]**2+1)**0.5])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3mtaczanx2Z"
      },
      "source": [
        "def compute_D_k(x):\r\n",
        "  assert type(x) is np.ndarray\r\n",
        "  assert len(x) == 2\r\n",
        "  if np.linalg.det(evalh(x)) == 0:\r\n",
        "    raise ValueError('Determinant does not exist. Please check!!')\r\n",
        "  return np.linalg.inv(evalh(x))  #computing inverse of Hessian."
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcSzBmddn09G"
      },
      "source": [
        "def compute_steplength_backtracking_Newton_method(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  #assert type(direction) is np.ndarray and len(direction) == 2 \r\n",
        "  assert type(alpha_start) is float and alpha_start>=0. \r\n",
        "  assert type(rho) is float and rho>=0.\r\n",
        "  assert type(gamma) is float and gamma>=0. \r\n",
        "  alpha = alpha_start\r\n",
        "  D_k = compute_D_k(x)\r\n",
        "  while evalf(x + alpha*np.matmul(D_k,-gradf)) > evalf(x) + gamma*alpha*(np.matmul(np.matrix.transpose(gradf), np.matmul(D_k,-gradf)) ):\r\n",
        "    alpha = rho*alpha\r\n",
        "  return alpha "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcWQEwnun5eX"
      },
      "source": [
        "#line search type \r\n",
        "CONSTANT_STEP_LENGTH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "BACKTRACKING_LINE_SEARCH_SCALING = 3"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USM4ej6Hn9K4"
      },
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\r\n",
        "\r\n",
        "def find_minimizer_Newton_method(start_x, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "\r\n",
        "  #initialization for backtracking line search\r\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2]\r\n",
        "\r\n",
        "  k = 0\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "    D_k = compute_D_k(x)\r\n",
        "    import scipy\r\n",
        "    from scipy.linalg import sqrtm\r\n",
        "    d = scipy.linalg.sqrtm(D_k)\r\n",
        "    if line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 1.0\r\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking_Newton_method(x, g_x, alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED') \r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')    \r\n",
        "    #implement the gradient descent steps here  \r\n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x))) #update x = x - step_length*g_x\r\n",
        "    k += 1 #increment iteration\r\n",
        "    #print('iter:',k)\r\n",
        "    g_x = evalg(x) #compute gradient at new point\r\n",
        "  return x, k, evalf(x)  "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32OeIw3loAjs"
      },
      "source": [
        "my_start_x = np.array([1.0,1.0])\r\n",
        "my_tol= 1e-9"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNmaupIN4g6"
      },
      "source": [
        "**2.ANSWER:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04ohqJgaoYUI",
        "outputId": "ee3bf44c-8a5e-460d-c951-8814f62faa56"
      },
      "source": [
        "print(\"For Newton's Method with CONSTANT_STEP_LENGTH procedure :\")\r\n",
        "x_opt, k, f_value = find_minimizer_Newton_method(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\r\n",
        "print(\"Minimizer = \",x_opt,\",  Iteration = \",k,\",  Minimum function value = \", f_value) \r\n",
        "\r\n",
        "print(\"\\nFor Newton's Method with BACKTRACKING_LINE_SEARCH :\")\r\n",
        "x_opt_bls, k, f_value = find_minimizer_Newton_method(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.0, 0.5,0.5)\r\n",
        "print(\"Minimizer = \",x_opt_bls,\",  Iteration = \", k , \",  Minimum function value = \",f_value)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Newton's Method with CONSTANT_STEP_LENGTH procedure :\n",
            "Minimizer =  [3.93389978e-12 3.93389978e-12] ,  Iteration =  36 ,  Minimum function value =  2.0\n",
            "\n",
            "For Newton's Method with BACKTRACKING_LINE_SEARCH :\n",
            "Minimizer =  [2.22044605e-16 2.22044605e-16] ,  Iteration =  1 ,  Minimum function value =  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ49vc2JOAWV"
      },
      "source": [
        "**COMMENTS:** \\\r\n",
        "1. From the above output it is clear that , **Newton's Method with BACKTRACKING_LINE_SEARCH (number of iterations = 1)** converges with a much faster rate than the **Newton's Method with CONSTANT_STEP_LENGTH procedure (number of iterations = 36).** \\\r\n",
        "2. Minimum function value remains same in both the cases i.e. 2.0. \\\r\n",
        "3. For both cases Minimizer is different i.e. point of convergence is different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqzPRQkxQj6K"
      },
      "source": [
        "**3. SOLUTION:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7ngBmcPp_tr"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search without scaling.\r\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  \r\n",
        "  alpha = alpha_start\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  #implement the backtracking line search\r\n",
        "  while evalf(np.add(x,-alpha*gradf)) > evalf(x)-gamma*alpha*np.matmul(gr_t, gradf):\r\n",
        "    alpha = rho*alpha\r\n",
        "  #print('final step length:',alpha)\r\n",
        "  return alpha"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1sYnzqHtnlw"
      },
      "source": [
        "#complete the code for gradient descent to find the minimizer\r\n",
        "def find_minimizer_gd(start_x, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  # construct a suitable A matrix for the quadratic function \r\n",
        "  x = start_x\r\n",
        "  A = (1/2)*evalh(x)\r\n",
        "  g_x = evalg(x)\r\n",
        "\r\n",
        "  #initialization for backtracking line search\r\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2]\r\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\r\n",
        "\r\n",
        "  k = 0\r\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "  \r\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 0.1\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    \r\n",
        "    #implement the gradient descent steps here   \r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x) #compute gradient at new point\r\n",
        "\r\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  return x , k , evalf(x)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgxutKE-QxJJ"
      },
      "source": [
        "**3. ANSWER:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws9qbi1uwLIE",
        "outputId": "fca44783-a1a7-49d7-b8dc-6da6ea11ed15"
      },
      "source": [
        "#check gradient descent (without scaling) with backtracking line search with starting point (1, 1).\r\n",
        "print(\"\\nFor BACKTRACKING_LINE_SEARCH WITHOUT_SCALING :\")\r\n",
        "x_opt_bls, k, f_value = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.0, 0.5,0.5)\r\n",
        "print(\"Minimizer = \",x_opt_bls,\",  Iteration = \", k , \",   Minimum function value = \",f_value)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For BACKTRACKING_LINE_SEARCH WITHOUT_SCALING :\n",
            "Minimizer =  [2.78991477e-19 2.78991477e-19] ,  Iteration =  4 ,   Minimum function value =  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDZiAgwrQq7C"
      },
      "source": [
        "**COMMENTS:** \\\r\n",
        "1. Here the number of Iterations required to converge to the minimum value is 4 which is greater than the number of iterations in Newton's Method with BACKTRACKING_LINE_SEARCH (=1) and less than the number of iterations in Newton's Method with CONSTANT_STEP_LENGTH procedure (=36). \\\r\n",
        "2. Minimum function value is same in all the cases (= 2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVIeVnXHz0AS"
      },
      "source": [
        "**4.SOLUTION:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHivcLVNzyjC"
      },
      "source": [
        "my_start_x = np.array([10.0, 10.0])\r\n",
        "my_tol= 1e-9"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6PJGWE47iJv",
        "outputId": "303d48ed-4569-4d8f-da93-3a0e62ca6d0f"
      },
      "source": [
        "print(\"\\nFor Newton's Method with BACKTRACKING_LINE_SEARCH :\")\r\n",
        "x_opt_bls, k, f_value = find_minimizer_Newton_method(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.0, 0.5,0.5)\r\n",
        "print(\"Minimizer = \",x_opt_bls,\",  Iteration = \", k , \",  Minimum function value = \",f_value)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For Newton's Method with BACKTRACKING_LINE_SEARCH :\n",
            "Minimizer =  [-9.92761578e-15 -9.92761578e-15] ,  Iteration =  17 ,  Minimum function value =  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hAfj5WBCy_D1",
        "outputId": "ac9c95ad-c164-4000-aff5-b2cc0f398644"
      },
      "source": [
        "print(\"For Newton's Method with CONSTANT_STEP_LENGTH procedure :\")\r\n",
        "x_opt, k, f_value = find_minimizer_Newton_method(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\r\n",
        "print(\"Minimizer = \",x_opt,\",  Iteration = \",k,\",  Minimum function value = \", f_value) \r\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Newton's Method with CONSTANT_STEP_LENGTH procedure :\n",
            "iter: 1\n",
            "iter: 2\n",
            "iter: 3\n",
            "iter: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-a48170c781a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"For Newton's Method with CONSTANT_STEP_LENGTH procedure :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_minimizer_Newton_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_start_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONSTANT_STEP_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minimizer = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",  Iteration = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",  Minimum function value = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-2041607a1223>\u001b[0m in \u001b[0;36mfind_minimizer_Newton_method\u001b[0;34m(start_x, tol, line_search_type, *args)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#continue as long as the norm of gradient is not close to zero upto a tolerance tol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mD_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_D_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqrtm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-e01dd25157e3>\u001b[0m in \u001b[0;36mcompute_D_k\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Determinant does not exist. Please check!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#computing inverse of Hessian.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Determinant does not exist. Please check!!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BcRFYXhL_5J"
      },
      "source": [
        "**REASON FOR ERROR MESSAGE:** \\\r\n",
        "Here, at the 5th iteration the value of the $Hessian = \\nabla^2 q(\\mathbf{x})$ is zero, i.e. Hessian becomes a singular matrix $\\Rightarrow$ its inverse is not possible. Therefore, we cannot proceed further With Newton's Method with CONSTANT_STEP_LENGTH procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpSo-dmaUN8Z"
      },
      "source": [
        "**COMMENTS:** \\\r\n",
        "1. For Newton's Method with CONSTANT_STEP_LENGTH procedure, at the 5th iteration the Hessian becomes Zero.\r\n",
        "2. For Newton's Method with BACKTRACKING_LINE_SEARCH the number of iterations required to converge were 17."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLbvpbUAB4CF"
      },
      "source": [
        "**5.ANSWER :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b9g9lS50wz9",
        "outputId": "c2715306-88d0-4bd3-cb82-29b426f5cb4e"
      },
      "source": [
        "#check gradient descent without scaling and with backtracking line search \r\n",
        "print(\"\\nFor BACKTRACKING_LINE_SEARCH WITHOUT_SCALING :\")\r\n",
        "x_opt_bls, k, f_value = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1.0, 0.5,0.5)\r\n",
        "print(\"Minimizer = \",x_opt_bls,\",  Iteration = \", k , \",   Minimum function value = \",f_value)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "For BACKTRACKING_LINE_SEARCH WITHOUT_SCALING :\n",
            "Minimizer =  [2.12455853e-14 2.12455853e-14] ,  Iteration =  13 ,   Minimum function value =  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5H4G2nITBCv"
      },
      "source": [
        "**COMMENTS:** \\\r\n",
        "1. In the previous question the number of iterations for Newton's Method with BACKTRACKING_LINE_SEARCH were 17 whereas in this question the number of iterations for gradient descent without scaling and with backtracking line search is little bit less i.e. 13.\\\r\n",
        "2. Minimum function value is same in both cases, equal to 2. But mimimizer is different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2TfKGk3Nz4W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}