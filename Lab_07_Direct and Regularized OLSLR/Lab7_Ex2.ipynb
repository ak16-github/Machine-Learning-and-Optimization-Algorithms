{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190011_IE684_Lab7_Ex2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCz6oDazwCz"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \r\n",
        "def evalf(x, n, lam):  \r\n",
        "  #Input: x is a numpy array of size n \r\n",
        "  assert type(x) is np.ndarray  \r\n",
        "  assert len(x) == n \r\n",
        "  return 0.5*(np.linalg.norm(np.matmul(A,x) - y))**2 + 0.5*lam*np.matmul(x.T,x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfaRDL7ItpjQ"
      },
      "source": [
        "def evalg(x, n, lam):\r\n",
        "  assert type(x) is np.ndarray\r\n",
        "  assert len(x) == n\r\n",
        "  return lam*x + np.matmul(A.T, np.matmul(A, x) - y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVjLvlzDxXRJ"
      },
      "source": [
        "def evalh(x,n,lam):\r\n",
        "  assert type(x) is np.ndarray  #do not allow arbitrary type arguments \r\n",
        "  assert len(x) == n #do not allow arbitrary size arguments \r\n",
        "  return lam*np.eye(n) + np.matmul(A.T, A)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2cIGt54xczm"
      },
      "source": [
        "#line search type \r\n",
        "EXACT_LINE_SEARCH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P3VSaTFxjdp"
      },
      "source": [
        "def compute_steplength_backtracking_scaled(x,n,lam, gradf, direction, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(x) == n\r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == n\r\n",
        "  #assert type(direction) is np.ndarray and len(direction) == 2  \r\n",
        "   \r\n",
        "  #assert type(alpha_start) is float and alpha_start>=0. \r\n",
        "  assert type(rho) is float and rho>=0.\r\n",
        "  assert type(gamma) is float and gamma>=0. \r\n",
        "   \r\n",
        "  #Complete the code \r\n",
        "  alpha = alpha_start\r\n",
        "  gradf = evalg(x,n,lam)\r\n",
        "  p=direction\r\n",
        "  #np.matmul(np.matrix.transpose(gradf), p)\r\n",
        "  while evalf(x+alpha*p,n,lam) > evalf(x,n,lam) + gamma*alpha*np.matmul(np.matrix.transpose(gradf), p) :\r\n",
        "    alpha = rho*alpha \r\n",
        "  return alpha"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqk4bm56Bkfl"
      },
      "source": [
        "import math"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUEI7557xm83"
      },
      "source": [
        "def find_minimizer_Newtonmethod(start_x, n,lam, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray #do not allow arbitrary type arguments \r\n",
        "  assert len(start_x) == n #do not allow arbitrary size arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  \r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x,n,lam)\r\n",
        "  h_x = evalh(x,n,lam)\r\n",
        "\r\n",
        "  if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "    if args is None:\r\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any args. Please check!'\r\n",
        "      raise ValueError(err_msg)\r\n",
        "    elif len(args)<3 :\r\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three args. Please check!'\r\n",
        "      raise ValueError(err_msg)\r\n",
        "    else:\r\n",
        "      alpha_start = float(args[0])\r\n",
        "      rho = float(args[1])\r\n",
        "      gamma = float(args[2])\r\n",
        "  k = 0\r\n",
        "  \r\n",
        "  #print('iter:',k,  ' f(x):', evalf(x,n), ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  x_newton =  []\r\n",
        "  f_newton = []\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "    #implement the Newton's method here\r\n",
        "    D_k=np.linalg.inv(evalh(x,n,lam))\r\n",
        "    direction = np.matmul(D_k,-g_x)\r\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking_scaled(x,n,lam,g_x, direction, alpha_start, rho, gamma)  \r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 1.0\r\n",
        "      \r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    #x_newton.append(math.log(np.linalg.norm(x - x_bar)))\r\n",
        "    #f_newton.append(math.log(np.linalg.norm(evalf(x,n,lam) - evalf(x_bar,n,lam))))\r\n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x)))\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x, n,lam) #compute gradient at new point\r\n",
        "    \r\n",
        "  return x, evalf(x,n,lam)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA-LYOCOxpJY"
      },
      "source": [
        "def find_minimizer_BFGS_method(start_x, n,lam, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray #do not allow arbitrary type arguments \r\n",
        "  assert len(start_x) == n #do not allow arbitrary size arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  \r\n",
        "  x = start_x\r\n",
        "  x0 = x\r\n",
        "  g_x = evalg(x,n,lam)\r\n",
        "  g0 = g_x\r\n",
        "\r\n",
        "  \r\n",
        "  if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "    if args is None:\r\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any args. Please check!'\r\n",
        "      raise ValueError(err_msg)\r\n",
        "    elif len(args)<3 :\r\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three args. Please check!'\r\n",
        "      raise ValueError(err_msg)\r\n",
        "    else:\r\n",
        "      alpha_start = float(args[0])\r\n",
        "      rho = float(args[1])\r\n",
        "      gamma = float(args[2])\r\n",
        "  k = 0\r\n",
        "  \r\n",
        "  #print('iter:',k,  ' f(x):', evalf(x,n), ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  x_bfgs = []\r\n",
        "  f_bfgs = []\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "    #implement the Newton's method here\r\n",
        "    \r\n",
        "    x0 = x\r\n",
        "    g_x = evalg(x,n,lam)\r\n",
        "    g0 = g_x\r\n",
        "\r\n",
        "    if k==0:\r\n",
        "      B_k=np.identity(n)\r\n",
        "    else:\r\n",
        "\r\n",
        "      I = np.identity(n)\r\n",
        "      \r\n",
        "      mu_k = 1/np.matmul(np.transpose(y_k),s_k)\r\n",
        "\r\n",
        "      B_k = np.add(np.matmul(np.matmul(np.subtract(I, mu_k*np.outer( s_k, np.transpose(y_k))),B_k), np.subtract(I, mu_k*np.outer(y_k,np.transpose(s_k)))), mu_k*np.outer( s_k, np.transpose(s_k)))\r\n",
        "    direction = np.matmul(B_k,-g_x)\r\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking_scaled(x,n,lam,g_x, direction, alpha_start, rho, gamma)     \r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 1.0\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    #x_bfgs.append(math.log(np.linalg.norm(x0 - x_bar)))\r\n",
        "    #f_bfgs.append(math.log(np.linalg.norm(evalf(x0,n,lam) - evalf(x_bar,n,lam))))\r\n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(B_k, g_x)))\r\n",
        "    g_x = evalg(x, n,lam)\r\n",
        "    s_k = x-x0 \r\n",
        "    y_k=  g_x-g0\r\n",
        "    k += 1 #increment iteration\r\n",
        "    #g_x = evalg(x, n,lam) #compute gradient at new point\r\n",
        "   \r\n",
        "  return x, evalf(x,n,lam)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEr_lX4j1uln"
      },
      "source": [
        "alpha = 0.9\r\n",
        "rho = 0.5\r\n",
        "gamma = 0.5\r\n",
        "my_tol= 1e-5\r\n",
        "lam=0.1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oLvu0gczUIU",
        "outputId": "b7285562-7f75-4111-b296-139777a67916"
      },
      "source": [
        "#Code for Newton method\r\n",
        "import numpy as np\r\n",
        "import timeit\r\n",
        "np.random.seed(1000) #for repeatability\r\n",
        "N = 200\r\n",
        "ds = [1000, 5000, 10000, 20000, 25000, 50000, 100000, 200000, 500000, 1000000]\r\n",
        "lambda_reg = 0.1\r\n",
        "eps = np.random.randn(N,1) #random noise\r\n",
        "#For each value of dimension in the ds array, we will check the behavior of Newton method\r\n",
        "for i in range(np.size(ds)):\r\n",
        "  d=ds[i]\r\n",
        "  A = np.random.randn(N,d)\r\n",
        "  xorig = np.ones((d,1))\r\n",
        "  my_start_x = np.zeros((d,1))\r\n",
        "  y = np.dot(A,xorig) + eps\r\n",
        "  start = timeit.default_timer()\r\n",
        "  #call Newton method with A,y,lambda and obtain the optimal solution x_opt\r\n",
        "  #x_opt = Newton(A,y,lambda_reg)\r\n",
        "  x_opt, opt_fval = find_minimizer_Newtonmethod(my_start_x, d,lam, my_tol, BACKTRACKING_LINE_SEARCH, alpha, rho, gamma)\r\n",
        "  newtontime = timeit.default_timer() - start #time is in seconds\r\n",
        "  #print the total time and the L2 norm difference || x_opt - xorig|| for Newton method\r\n",
        "  diff = np.subtract(x_opt, xorig)\r\n",
        "  print(\"\\n\\nd:\", d)\r\n",
        "  print(\"Total time:\", newtontime)\r\n",
        "  print(\"L2 norm difference ||Ax* - y||_2^2: \", np.linalg.norm(np.subtract(np.matmul(A, x_opt), y))**2)\r\n",
        "  print(\"L2 norm difference || x_opt - xorig||: \", np.linalg.norm(diff)**2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "d: 1000\n",
            "Total time: 1.4444547960001728\n",
            "L2 norm difference ||Ax* - y||_2^2:  0.0021220541764787873\n",
            "L2 norm difference || x_opt - xorig||:  817.7202119606469\n",
            "\n",
            "\n",
            "d: 5000\n",
            "Total time: 112.31632143199977\n",
            "L2 norm difference ||Ax* - y||_2^2:  0.00047397442649197916\n",
            "L2 norm difference || x_opt - xorig||:  4774.913361486921\n",
            "\n",
            "\n",
            "d: 10000\n",
            "Total time: 962.785232056\n",
            "L2 norm difference ||Ax* - y||_2^2:  0.00017517588089034528\n",
            "L2 norm difference || x_opt - xorig||:  9825.861491699694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "5Upf431y-pap",
        "outputId": "a1e55817-31dd-4e90-e6a6-62316203ccbc"
      },
      "source": [
        "import pandas as pd\r\n",
        "Dimention_of_A=[(200, 1000),(200,5000),(200,10000)]\r\n",
        "Total_Time = [1.4444547960001728, 112.31632143199977, 962.785232056]\r\n",
        "L2_norm_difference1= [ 0.0021220541764787873, 0.00047397442649197916,  0.00017517588089034528]\r\n",
        "L2_norm_difference2= [817.7202119606469, 4774.913361486921 , 9825.861491699694]\r\n",
        "data = {'Dimention_of_A ': Dimention_of_A,'Total Time':Total_Time,'||Ax^* - y||2^2':L2_norm_difference1,'||x^* - x{orig}||_2^2':L2_norm_difference2}\r\n",
        "df = pd.DataFrame(data)\r\n",
        "df"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dimention_of_A</th>\n",
              "      <th>Total Time</th>\n",
              "      <th>||Ax^* - y||2^2</th>\n",
              "      <th>||x^* - x{orig}||_2^2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(200, 1000)</td>\n",
              "      <td>1.444455</td>\n",
              "      <td>0.002122</td>\n",
              "      <td>817.720212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(200, 5000)</td>\n",
              "      <td>112.316321</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>4774.913361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(200, 10000)</td>\n",
              "      <td>962.785232</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>9825.861492</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Dimention_of_A   Total Time  ||Ax^* - y||2^2  ||x^* - x{orig}||_2^2\n",
              "0     (200, 1000)    1.444455         0.002122             817.720212\n",
              "1     (200, 5000)  112.316321         0.000474            4774.913361\n",
              "2    (200, 10000)  962.785232         0.000175            9825.861492"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij0rg1462qKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d222b86-d55b-475f-87a1-549d5c96af33"
      },
      "source": [
        "#Code for BFGS method\r\n",
        "import numpy as np\r\n",
        "import timeit\r\n",
        "np.random.seed(1000) #for repeatability\r\n",
        "N = 200\r\n",
        "ds = [1000, 5000, 10000, 20000, 25000, 50000, 100000, 200000, 500000, 1000000]\r\n",
        "lambda_reg = 0.1\r\n",
        "eps = np.random.randn(N,1) #random noise\r\n",
        "#For each value of dimension in the ds array, we will check the behavior of BFGS method\r\n",
        "for i in range(np.size(ds)):\r\n",
        "  d=ds[i]\r\n",
        "  A = np.random.randn(N,d)\r\n",
        "  xorig = np.ones((d,1))\r\n",
        "  my_start_x = np.zeros((d,1))\r\n",
        "  y = np.dot(A,xorig) + eps\r\n",
        "  start = timeit.default_timer()\r\n",
        "  #call Newton method with A,y,lambda and obtain the optimal solution x_opt\r\n",
        "  #x_opt = Newton(A,y,lambda_reg)\r\n",
        "  newtontime = timeit.default_timer() - start #time is in seconds\r\n",
        "  #print the total time, ||Ax_opt-y||^2 and the L2 norm difference || x_opt - xorig||^2 for Newton method  \r\n",
        "  start = timeit.default_timer()\r\n",
        "  #call BFGS method with A,y,lambda and obtain the optimal solution x_opt_bfgs\r\n",
        "  #x_opt_bfgs = BFGS(A,y,lambda_reg)\r\n",
        "  x_opt_bfgs, opt_fval_bfgs = find_minimizer_BFGS_method(my_start_x, d,lam,my_tol,BACKTRACKING_LINE_SEARCH,0.9, 0.5,0.5)\r\n",
        "  bfgstime = timeit.default_timer() - start #time is in seconds\r\n",
        "  #print the total time, ||Ax_opt_bfgs-y||^2 and the L2 norm difference || x_opt_bfgs - xorig||^2 for BFGS method\r\n",
        "  diff = np.subtract(x_opt_bfgs, xorig)\r\n",
        "  print(\"d:\", d)\r\n",
        "  print(\"Total time:\", bfgstime)\r\n",
        "  print(\"L2 norm difference ||Ax* - y||_2^2: \", np.linalg.norm(np.subtract(np.matmul(A, x_opt_bfgs), y))**2)\r\n",
        "  print(\"L2 norm difference || x_opt - xorig||: \", np.linalg.norm(diff)**2)\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "d: 1000\n",
            "Total time: 21.82778980799958\n",
            "L2 norm difference ||Ax* - y||_2^2:  0.0021220537342639904\n",
            "L2 norm difference || x_opt - xorig||:  817.7202119597779\n",
            "d: 5000\n",
            "Total time: 1778.1315902699998\n",
            "L2 norm difference ||Ax* - y||_2^2:  0.0004739707628245362\n",
            "L2 norm difference || x_opt - xorig||:  4774.913361486827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV2zeWL-31SQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "644a8c24-edaf-44e5-bfe2-a6fc61029f00"
      },
      "source": [
        "import pandas as pd\r\n",
        "Dimention_of_A=[(200, 1000),(200,5000)]\r\n",
        "Total_Time = [21.82778980799958, 1778.1315902699998]\r\n",
        "L2_norm_difference1= [ 0.0021220537342639904, 0.0004739707628245362]\r\n",
        "L2_norm_difference2= [817.7202119597779, 4774.913361486827]\r\n",
        "data2 = {'Dimention_of_A ': Dimention_of_A,'Total Time':Total_Time,'||Ax^* - y||2^2':L2_norm_difference1,'||x^* - x{orig}||_2^2':L2_norm_difference2}\r\n",
        "df2 = pd.DataFrame(data2)\r\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dimention_of_A</th>\n",
              "      <th>Total Time</th>\n",
              "      <th>||Ax^* - y||2^2</th>\n",
              "      <th>||x^* - x{orig}||_2^2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(200, 1000)</td>\n",
              "      <td>21.82779</td>\n",
              "      <td>0.002122</td>\n",
              "      <td>817.720212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(200, 5000)</td>\n",
              "      <td>1778.13159</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>4774.913361</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Dimention_of_A   Total Time  ||Ax^* - y||2^2  ||x^* - x{orig}||_2^2\n",
              "0     (200, 1000)    21.82779         0.002122             817.720212\n",
              "1     (200, 5000)  1778.13159         0.000474            4774.913361"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_Sk10ZbY4VF"
      },
      "source": [
        "**We are able to find the minimum using both the mathod regularized OLSLR and Direct OLSLR as hessian never becomes non-invertible (singular matrix) and we do run into trouble because of approximation of hessian. Reguralized OLSLR makes approximated hessian positive definite by adding lambda times identity and hence we are able to find the hessian inverse and then we find the minimum.**"
      ]
    }
  ]
}