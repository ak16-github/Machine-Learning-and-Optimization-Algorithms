{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 20i190011_IE684_Lab7_EX1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNMW0yzBLNhV"
      },
      "source": [
        "$\\textbf{Direct OLSLR}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWLt3FpUoAU9"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.datasets import load_digits"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeMx4GA6oNKm",
        "outputId": "f6cfefce-b05f-43e0-8bba-9eeceb0dc78a"
      },
      "source": [
        "digits = load_digits()\r\n",
        "#check the shape of digits data\r\n",
        "print(digits.data.shape)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1797, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e79iEpsmoRc8",
        "outputId": "31ea5f4f-8e19-4706-d860-b2bb91798d5a"
      },
      "source": [
        "#check the shape of digits target\r\n",
        "print(digits.target.shape)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1797,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5lu_OhoXFX"
      },
      "source": [
        "#let us use the linear regression used in the previous lab\r\n",
        "N = digits.data.shape[0] #Number of data points\r\n",
        "n = digits.data.shape[1] #Dimension of data points\r\n",
        "A = digits.data"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_l33xM3oaH0"
      },
      "source": [
        "#In the following code, we create a Nx1 vector of target labels\r\n",
        "y = 1.0*np.ones([A.shape[0],1])\r\n",
        "for i in range(digits.target.shape[0]):\r\n",
        "  y[i] = digits.target[i]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA1zWRO0xMd6"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x, n):  \n",
        "  #Input: x is a numpy array of size n \n",
        "  assert type(x) is np.ndarray  #do not allow arbitrary type arguments \n",
        "  assert len(x) == n #do not allow arbitrary size arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  #A = np.random.randn(800, 2)\n",
        "  #xy_min = [-4, -2]\n",
        "  #xy_max = [6, 8]\n",
        "  #x_bar = np.random.uniform(low=xy_min, high=xy_max, size=(1,2))\n",
        "  #x_bar = x_bar.T\n",
        "  #epsilon = np.random.randn(800, 1)\n",
        "  #A_x = np.matmul(A, x_bar)\n",
        "  #y = A_x + epsilon\n",
        "  fval = np.linalg.norm(np.matmul(A,x) - y)\n",
        "  fval = 0.5 * (fval)**2\n",
        "  return (fval)\n",
        "\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoXOhsU5rvzM"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \r\n",
        "def evalg(x, n):\r\n",
        "  assert type(x) is np.ndarray \r\n",
        "  assert len(x) == n\r\n",
        "  return np.matmul(A.T, np.matmul(A, x) - y)\r\n"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDua4UGVr-2s"
      },
      "source": [
        "def evalh(x,n):\r\n",
        "  assert type(x) is np.ndarray  #do not allow arbitrary type arguments \r\n",
        "  assert len(x) == n #do not allow arbitrary size arguments \r\n",
        "  return np.matmul(A.T,A)\r\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G6yneyAIXs8"
      },
      "source": [
        "def compute_B_k(s, y, B_k, n):\n",
        "  assert type(s) is np.ndarray #do not allow arbitrary type arguments \n",
        "  assert type(y) is np.ndarray #do not allow arbitrary type arguments \n",
        "  assert type(B_k) is np.ndarray #do not allow arbitrary type arguments \n",
        "  assert len(s) == n and len(y) == n #do not allow arbitrary size arguments \n",
        "  mu = 1 / np.dot(np.squeeze(np.asarray(y)), np.squeeze(np.asarray(s)))\n",
        "  I = np.identity(n)\n",
        "  a1 = np.outer(np.multiply(mu, s), np.transpose(y))\n",
        "  a2 = np.outer(np.multiply(mu, y), np.transpose(s))\n",
        "  a = np.subtract(I, a1)\n",
        "  b = np.subtract(I, a2)\n",
        "  c = np.outer(np.multiply(mu, s), np.transpose(s))\n",
        "  d = np.matmul(np.matmul(a, B_k), b)\n",
        "  z = np.add(d, c)\n",
        "  return z"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW36gGxBF0fL"
      },
      "source": [
        "#line search type \n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZojFZ_nd-Ol"
      },
      "source": [
        "def compute_D_k(x,n):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == n\n",
        "  mat = evalh(x,n)\n",
        "  return np.linalg.inv(mat)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV3GZGcJZwhL"
      },
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, alpha_start, rho, gamma, B_k): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray\n",
        "  assert type(gradf) is np.ndarray\n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  n = len(x)\n",
        "  alpha = alpha_start\n",
        "  p = - gradf\n",
        "  D_k = B_k\n",
        "  #implement the backtracking line search\n",
        "  #while evalf(x + alpha*p, n) > evalf(x, n) + gamma * alpha* (np.matmul(np.matrix.transpose(p), p)):\n",
        "  while evalf(x + alpha*np.matmul(D_k,p), n) > evalf(x, n) + gamma * alpha* (np.matmul(np.matrix.transpose(gradf), np.matmul(D_k,p)) ):\n",
        "    alpha = alpha * rho\n",
        "  return alpha  "
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kevIm2u3xleX"
      },
      "source": [
        "def find_minimizer_Newtonmethod(start_x, n, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray #do not allow arbitrary type arguments \n",
        "  assert len(start_x) == n #do not allow arbitrary size arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  \n",
        "  x = start_x\n",
        "  g_x = evalg(x,n)\n",
        "  h_x = evalh(x,n)\n",
        "\n",
        "  if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "    if args is None:\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any args. Please check!'\n",
        "      raise ValueError(err_msg)\n",
        "    elif len(args)<3 :\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three args. Please check!'\n",
        "      raise ValueError(err_msg)\n",
        "    else:\n",
        "      alpha_start = float(args[0])\n",
        "      rho = float(args[1])\n",
        "      gamma = float(args[2])\n",
        "  k = 0\n",
        "  \n",
        "  #print('iter:',k,  ' f(x):', evalf(x,n), ' gradient norm:', np.linalg.norm(g_x))\n",
        "  step_length = 1.\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    #implement the Newton's method here\n",
        "    D_k = np.linalg.inv(evalh(x, n))\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x, g_x, alpha_start, rho, gamma, D_k)\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1.\n",
        "    else:\n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x, n) #compute gradient at new point\n",
        "  return x,  k"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1MErjxAZ410"
      },
      "source": [
        "def find_minimizer_BFGS_scaling(start_x, n, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray #do not allow arbitrary type arguments \n",
        "  assert len(start_x) == n #do not allow arbitrary size arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  \n",
        "  x = start_x\n",
        "  g_x = evalg(x,n)\n",
        "\n",
        "  if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "    if args is None:\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any args. Please check!'\n",
        "      raise ValueError(err_msg)\n",
        "    elif len(args)<3 :\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three args. Please check!'\n",
        "      raise ValueError(err_msg)\n",
        "    else:\n",
        "      alpha_start = float(args[0])\n",
        "      rho = float(args[1])\n",
        "      gamma = float(args[2])\n",
        "\n",
        "  k = 0\n",
        "  B_k = np.identity(n)\n",
        "  x_old = x\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x_old, g_x, alpha_start, rho, gamma, B_k)\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1.\n",
        "    else:\n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "\n",
        "    x_new = np.subtract(x_old, np.multiply(step_length, np.matmul(B_k,g_x))) \n",
        "    g_x = evalg(x_new,n)\n",
        "    s = np.subtract(x_new, x_old)\n",
        "    y = np.subtract(evalg(x_new,n), evalg(x_old,n)) \n",
        "    B_k = compute_B_k(s, y, B_k, n)\n",
        "    x_old = x_new\n",
        "    k += 1\n",
        "  return x_new, k"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqufESO8HMYZ"
      },
      "source": [
        "**Question.1 (Partial solution)- for direct OLSLR >>> using Newton method (BACKTRACKING_LINE_SEARCH).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "bNFF0CQsZ56y",
        "outputId": "2d825f8b-8908-42f0-8009-c896ec9a07b9"
      },
      "source": [
        "alpha = 0.9\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "my_tol= 1e-5\n",
        "my_start_x =np.zeros((n, 1))\n",
        "x, k = find_minimizer_Newtonmethod(my_start_x, n, my_tol, BACKTRACKING_LINE_SEARCH, alpha, rho, gamma)\n",
        "print(x, k)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-d6db1c64dac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_start_x\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_minimizer_Newtonmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_start_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-128-2b5ea17dfa00>\u001b[0m in \u001b[0;36mfind_minimizer_Newtonmethod\u001b[0;34m(start_x, n, tol, line_search_type, *args)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#continue as long as the norm of gradient is not close to zero upto a tolerance tol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#implement the Newton's method here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mD_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_steplength_backtracking_scaled_direction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiN2xkBnGe96"
      },
      "source": [
        "**Question.2 (Partial solution)- for direct OLSLR >>> using BFGS method (BACKTRACKING_LINE_SEARCH).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYPAfvWSw1Ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e355403-4fc0-492b-dea8-7955b4655d55"
      },
      "source": [
        "alpha = 0.9\r\n",
        "rho = 0.5\r\n",
        "gamma = 0.5\r\n",
        "my_tol= 1e-5\r\n",
        "my_start_x =np.zeros((64, 1))\r\n",
        "x, k = find_minimizer_BFGS_scaling(my_start_x, n, my_tol, BACKTRACKING_LINE_SEARCH, alpha, rho, gamma)\r\n",
        "print(x)\r\n"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.00000000e+00]\n",
            " [ 9.69033568e-02]\n",
            " [-4.32277232e-03]\n",
            " [-7.76028319e-03]\n",
            " [ 7.49594380e-02]\n",
            " [ 1.13947198e-02]\n",
            " [-2.71328245e-02]\n",
            " [-7.33176333e-03]\n",
            " [ 9.98337968e-01]\n",
            " [-2.88095538e-02]\n",
            " [ 1.18688288e-01]\n",
            " [ 6.60916265e-02]\n",
            " [-5.57069862e-02]\n",
            " [-6.97063705e-02]\n",
            " [ 9.65876439e-02]\n",
            " [ 2.55182251e-01]\n",
            " [-7.29828608e-01]\n",
            " [ 2.42709916e-02]\n",
            " [ 7.73249597e-02]\n",
            " [-2.33000278e-02]\n",
            " [-5.64086144e-02]\n",
            " [ 5.72426822e-02]\n",
            " [-4.88717684e-02]\n",
            " [-2.62467763e-01]\n",
            " [-9.06562829e-01]\n",
            " [-1.49767791e-01]\n",
            " [ 5.64019538e-02]\n",
            " [ 8.96663590e-02]\n",
            " [ 8.39318159e-02]\n",
            " [ 9.85411936e-02]\n",
            " [ 1.69317614e-03]\n",
            " [-2.96805758e+00]\n",
            " [ 0.00000000e+00]\n",
            " [-1.54362338e-01]\n",
            " [-9.32361205e-03]\n",
            " [ 1.39497628e-01]\n",
            " [-3.69234835e-02]\n",
            " [ 5.46111776e-02]\n",
            " [-9.20505070e-03]\n",
            " [ 0.00000000e+00]\n",
            " [ 1.03279535e-01]\n",
            " [ 1.23983258e-01]\n",
            " [-1.37639605e-02]\n",
            " [ 5.40087816e-03]\n",
            " [ 1.31185107e-01]\n",
            " [ 5.49570758e-02]\n",
            " [ 2.24938237e-02]\n",
            " [ 7.47977909e-03]\n",
            " [ 6.17755030e-01]\n",
            " [ 2.44122357e-02]\n",
            " [ 1.42333037e-03]\n",
            " [-6.21110760e-02]\n",
            " [-2.07025036e-01]\n",
            " [-3.38506003e-02]\n",
            " [ 1.05486736e-01]\n",
            " [-1.40335957e-01]\n",
            " [-9.84169005e-01]\n",
            " [-1.14467153e-01]\n",
            " [ 2.10494893e-02]\n",
            " [-4.36076105e-02]\n",
            " [ 1.87374934e-02]\n",
            " [-6.66567878e-02]\n",
            " [ 1.19382626e-02]\n",
            " [-5.27776612e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q87kqhrVK0dT"
      },
      "source": [
        "$\\textbf{Now, regularized OLSLR}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCz6oDazwCz"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \r\n",
        "def evalf(x, n, lam):  \r\n",
        "  #Input: x is a numpy array of size n \r\n",
        "  assert type(x) is np.ndarray  \r\n",
        "  assert len(x) == n \r\n",
        "  return 0.5*(np.linalg.norm(np.matmul(A,x) - y))**2 + 0.5*lam*np.matmul(x.T,x)"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfaRDL7ItpjQ"
      },
      "source": [
        "def evalg(x, n, lam):\r\n",
        "  assert type(x) is np.ndarray\r\n",
        "  assert len(x) == n\r\n",
        "  return lam*x + np.matmul(A.T, np.matmul(A, x) - y)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVjLvlzDxXRJ"
      },
      "source": [
        "def evalh(x,n,lam):\r\n",
        "  assert type(x) is np.ndarray  #do not allow arbitrary type arguments \r\n",
        "  assert len(x) == n #do not allow arbitrary size arguments \r\n",
        "  return lam*np.eye(n) + np.matmul(A.T, A)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P3VSaTFxjdp"
      },
      "source": [
        "def compute_steplength_backtracking_scaled(x,n,lam, gradf, direction, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(x) == n\r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == n\r\n",
        "  #assert type(direction) is np.ndarray and len(direction) == 2  \r\n",
        "   \r\n",
        "  #assert type(alpha_start) is float and alpha_start>=0. \r\n",
        "  assert type(rho) is float and rho>=0.\r\n",
        "  assert type(gamma) is float and gamma>=0. \r\n",
        "   \r\n",
        "  #Complete the code \r\n",
        "  alpha = alpha_start\r\n",
        "  gradf = evalg(x,n,lam)\r\n",
        "  p=direction\r\n",
        "  #np.matmul(np.matrix.transpose(gradf), p)\r\n",
        "  while evalf(x+alpha*p,n,lam) > evalf(x,n,lam) + gamma*alpha*np.matmul(np.matrix.transpose(gradf), p) :\r\n",
        "    alpha = rho*alpha \r\n",
        "  return alpha"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqk4bm56Bkfl"
      },
      "source": [
        "import math"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUEI7557xm83"
      },
      "source": [
        "def find_minimizer_Newtonmethod(start_x, n,lam, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray #do not allow arbitrary type arguments \r\n",
        "  assert len(start_x) == n #do not allow arbitrary size arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  \r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x,n,lam)\r\n",
        "  h_x = evalh(x,n,lam)\r\n",
        "\r\n",
        "  if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "    if args is None:\r\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any args. Please check!'\r\n",
        "      raise ValueError(err_msg)\r\n",
        "    elif len(args)<3 :\r\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three args. Please check!'\r\n",
        "      raise ValueError(err_msg)\r\n",
        "    else:\r\n",
        "      alpha_start = float(args[0])\r\n",
        "      rho = float(args[1])\r\n",
        "      gamma = float(args[2])\r\n",
        "  k = 0\r\n",
        "  \r\n",
        "  #print('iter:',k,  ' f(x):', evalf(x,n), ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  x_newton =  []\r\n",
        "  f_newton = []\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "    #implement the Newton's method here\r\n",
        "    D_k=np.linalg.inv(evalh(x,n,lam))\r\n",
        "    direction = np.matmul(D_k,-g_x)\r\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking_scaled(x,n,lam,g_x, direction, alpha_start, rho, gamma)  \r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 1.0\r\n",
        "      \r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    #x_newton.append(math.log(np.linalg.norm(x - x_bar)))\r\n",
        "    #f_newton.append(math.log(np.linalg.norm(evalf(x,n,lam) - evalf(x_bar,n,lam))))\r\n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k, g_x)))\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x, n,lam) #compute gradient at new point\r\n",
        "    \r\n",
        "  return x, evalf(x,n,lam), k,x_newton,f_newton"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA-LYOCOxpJY"
      },
      "source": [
        "def find_minimizer_BFGS_method(start_x, n,lam, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray #do not allow arbitrary type arguments \r\n",
        "  assert len(start_x) == n #do not allow arbitrary size arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  \r\n",
        "  x = start_x\r\n",
        "  x0 = x\r\n",
        "  g_x = evalg(x,n,lam)\r\n",
        "  g0 = g_x\r\n",
        "\r\n",
        "  \r\n",
        "  if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "    if args is None:\r\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any args. Please check!'\r\n",
        "      raise ValueError(err_msg)\r\n",
        "    elif len(args)<3 :\r\n",
        "      err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three args. Please check!'\r\n",
        "      raise ValueError(err_msg)\r\n",
        "    else:\r\n",
        "      alpha_start = float(args[0])\r\n",
        "      rho = float(args[1])\r\n",
        "      gamma = float(args[2])\r\n",
        "  k = 0\r\n",
        "  \r\n",
        "  #print('iter:',k,  ' f(x):', evalf(x,n), ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  x_bfgs = []\r\n",
        "  f_bfgs = []\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "    #implement the Newton's method here\r\n",
        "    \r\n",
        "    x0 = x\r\n",
        "    g_x = evalg(x,n,lam)\r\n",
        "    g0 = g_x\r\n",
        "\r\n",
        "    if k==0:\r\n",
        "      B_k=np.identity(n)\r\n",
        "    else:\r\n",
        "\r\n",
        "      I = np.identity(n)\r\n",
        "      \r\n",
        "      mu_k = 1/np.matmul(np.transpose(y_k),s_k)\r\n",
        "\r\n",
        "      B_k = np.add(np.matmul(np.matmul(np.subtract(I, mu_k*np.outer( s_k, np.transpose(y_k))),B_k), np.subtract(I, mu_k*np.outer(y_k,np.transpose(s_k)))), mu_k*np.outer( s_k, np.transpose(s_k)))\r\n",
        "    direction = np.matmul(B_k,-g_x)\r\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking_scaled(x,n,lam,g_x, direction, alpha_start, rho, gamma)     \r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 1.0\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    #x_bfgs.append(math.log(np.linalg.norm(x0 - x_bar)))\r\n",
        "    #f_bfgs.append(math.log(np.linalg.norm(evalf(x0,n,lam) - evalf(x_bar,n,lam))))\r\n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(B_k, g_x)))\r\n",
        "    g_x = evalg(x, n,lam)\r\n",
        "    s_k = x-x0 \r\n",
        "    y_k=  g_x-g0\r\n",
        "    k += 1 #increment iteration\r\n",
        "    #g_x = evalg(x, n,lam) #compute gradient at new point\r\n",
        "   \r\n",
        "  return x, evalf(x,n,lam), k,x_bfgs,f_bfgs"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWgQi3HTymwe"
      },
      "source": [
        "**EX.1 solution- for The $\\textbf{regularized OLSLR}$ with\r\n",
        "lambda = 0.1   >>>> NEWTON METHOD (BACKTRACKING_LINE_SEARCH)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTfBHQyvDUR3",
        "outputId": "ea2ca4c0-42e5-42b8-95ce-74544694c16c"
      },
      "source": [
        "alpha = 0.9\r\n",
        "rho = 0.5\r\n",
        "gamma = 0.5\r\n",
        "my_tol= 1e-4\r\n",
        "lam=0.1\r\n",
        "#Starting_Point=[np.zeros((2, 1)), np.array([[50],[50]]), np.array([[-5],[-5]])]\r\n",
        "my_start_x = np.zeros((n, 1))\r\n",
        "x, opt_fval, num_iters, x_newton,f_newton = find_minimizer_Newtonmethod(my_start_x, n,lam, my_tol, BACKTRACKING_LINE_SEARCH, alpha, rho, gamma)\r\n",
        "print(\"optimum_x_value is:\\n\",x)\r\n",
        "print(\"Number of Iterations:\",num_iters)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimum_x_value is:\n",
            " [[ 0.00000000e+00]\n",
            " [ 9.72176393e-02]\n",
            " [-4.25221013e-03]\n",
            " [-7.65725749e-03]\n",
            " [ 7.49359297e-02]\n",
            " [ 1.13924666e-02]\n",
            " [-2.68134810e-02]\n",
            " [-8.48370171e-03]\n",
            " [ 9.91208545e-01]\n",
            " [-2.87397984e-02]\n",
            " [ 1.18690196e-01]\n",
            " [ 6.61518400e-02]\n",
            " [-5.57615717e-02]\n",
            " [-6.96340237e-02]\n",
            " [ 9.62813519e-02]\n",
            " [ 2.56470858e-01]\n",
            " [-7.28979627e-01]\n",
            " [ 2.42825856e-02]\n",
            " [ 7.72526071e-02]\n",
            " [-2.33770172e-02]\n",
            " [-5.63320407e-02]\n",
            " [ 5.71246069e-02]\n",
            " [-4.84767009e-02]\n",
            " [-2.70744170e-01]\n",
            " [-8.60889237e-01]\n",
            " [-1.49941949e-01]\n",
            " [ 5.64334649e-02]\n",
            " [ 8.96806467e-02]\n",
            " [ 8.39114973e-02]\n",
            " [ 9.85243348e-02]\n",
            " [ 1.64759992e-03]\n",
            " [-2.82145749e+00]\n",
            " [ 0.00000000e+00]\n",
            " [-1.54275472e-01]\n",
            " [-9.36618641e-03]\n",
            " [ 1.39528972e-01]\n",
            " [-3.69438111e-02]\n",
            " [ 5.46098301e-02]\n",
            " [-9.13188784e-03]\n",
            " [ 0.00000000e+00]\n",
            " [ 1.07369006e-01]\n",
            " [ 1.23996365e-01]\n",
            " [-1.37231270e-02]\n",
            " [ 5.34871565e-03]\n",
            " [ 1.31237767e-01]\n",
            " [ 5.50202749e-02]\n",
            " [ 2.24738205e-02]\n",
            " [ 7.53480641e-03]\n",
            " [ 5.95009063e-01]\n",
            " [ 2.42332551e-02]\n",
            " [ 1.44538782e-03]\n",
            " [-6.21495531e-02]\n",
            " [-2.06985396e-01]\n",
            " [-3.38924139e-02]\n",
            " [ 1.05491772e-01]\n",
            " [-1.40375388e-01]\n",
            " [-8.25290583e-01]\n",
            " [-1.14990629e-01]\n",
            " [ 2.09661777e-02]\n",
            " [-4.36766596e-02]\n",
            " [ 1.87173457e-02]\n",
            " [-6.66050522e-02]\n",
            " [ 1.19523791e-02]\n",
            " [-5.28287670e-02]]\n",
            "Number of Iterations: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUW2CLTCV25n"
      },
      "source": [
        "**Here, we were not able to find the optimum value for tolerance = 10^(-5) because for that it was not converging and  thats why I have taken tolerance as  equal to 10^(-4), for which the optimum value of x is quite easily obtained in just 15 iterations.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf5DZpgIsXAe"
      },
      "source": [
        "**EX.2 solution- for The $\\textbf{regularized OLSLR}$ with\r\n",
        "lambda = 0.1   >>>> BFGS Method (BACKTRACKING_LINE_SEARCH)** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li2mD-JLEKPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5889817f-39c6-4a0f-e27b-293ad9630946"
      },
      "source": [
        "alpha = 0.9\r\n",
        "rho = 0.5\r\n",
        "gamma = 0.5\r\n",
        "my_tol= 1e-5\r\n",
        "lam=0.1\r\n",
        "x, opt_fval, num_iters, x_bfgs, f_bfgs= find_minimizer_BFGS_method(my_start_x,n,lam,my_tol,BACKTRACKING_LINE_SEARCH,0.9, 0.5,0.5)\r\n",
        "print(\"Optimum_X is:\")\r\n",
        "print(x)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimum_X is:\n",
            "[[ 0.00000000e+00]\n",
            " [ 9.72176393e-02]\n",
            " [-4.25221012e-03]\n",
            " [-7.65725749e-03]\n",
            " [ 7.49359298e-02]\n",
            " [ 1.13924666e-02]\n",
            " [-2.68134811e-02]\n",
            " [-8.48370171e-03]\n",
            " [ 9.91208545e-01]\n",
            " [-2.87397984e-02]\n",
            " [ 1.18690196e-01]\n",
            " [ 6.61518400e-02]\n",
            " [-5.57615717e-02]\n",
            " [-6.96340237e-02]\n",
            " [ 9.62813519e-02]\n",
            " [ 2.56470858e-01]\n",
            " [-7.28979628e-01]\n",
            " [ 2.42825856e-02]\n",
            " [ 7.72526071e-02]\n",
            " [-2.33770172e-02]\n",
            " [-5.63320407e-02]\n",
            " [ 5.71246069e-02]\n",
            " [-4.84767009e-02]\n",
            " [-2.70744170e-01]\n",
            " [-8.60889236e-01]\n",
            " [-1.49941949e-01]\n",
            " [ 5.64334649e-02]\n",
            " [ 8.96806467e-02]\n",
            " [ 8.39114973e-02]\n",
            " [ 9.85243348e-02]\n",
            " [ 1.64759992e-03]\n",
            " [-2.82145749e+00]\n",
            " [ 0.00000000e+00]\n",
            " [-1.54275472e-01]\n",
            " [-9.36618641e-03]\n",
            " [ 1.39528972e-01]\n",
            " [-3.69438111e-02]\n",
            " [ 5.46098301e-02]\n",
            " [-9.13188785e-03]\n",
            " [ 0.00000000e+00]\n",
            " [ 1.07369006e-01]\n",
            " [ 1.23996365e-01]\n",
            " [-1.37231270e-02]\n",
            " [ 5.34871565e-03]\n",
            " [ 1.31237767e-01]\n",
            " [ 5.50202750e-02]\n",
            " [ 2.24738205e-02]\n",
            " [ 7.53480639e-03]\n",
            " [ 5.95009063e-01]\n",
            " [ 2.42332551e-02]\n",
            " [ 1.44538782e-03]\n",
            " [-6.21495531e-02]\n",
            " [-2.06985396e-01]\n",
            " [-3.38924139e-02]\n",
            " [ 1.05491772e-01]\n",
            " [-1.40375388e-01]\n",
            " [-8.25290583e-01]\n",
            " [-1.14990629e-01]\n",
            " [ 2.09661777e-02]\n",
            " [-4.36766596e-02]\n",
            " [ 1.87173457e-02]\n",
            " [-6.66050522e-02]\n",
            " [ 1.19523791e-02]\n",
            " [-5.28287671e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nBwkz6rVKsr"
      },
      "source": [
        "**Here We are not able to find the optimum value using Direct OLSLR Newton method because hessian becomes non-invertible (singular matrix). But for Direct OLSLR BFGS method we are able to obtain the optimum values.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWFmuE6uVspv"
      },
      "source": [
        "**Also, from the above outputs it is obvious that in case of The regularized OLSLR Newton method we were not able to find the optimum value corresponding to the tolerance = 10^(-5) but for  tolerance it was easily obtained in just 15 iterations**"
      ]
    }
  ]
}