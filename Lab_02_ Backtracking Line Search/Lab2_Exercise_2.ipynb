{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190011_IE684_Lab2_Ex2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgYnO-JPeqZf"
      },
      "source": [
        "**Given function is :**\r\n",
        "\\begin{align}\r\n",
        "\\ h(x) = \\sum_{i=1}^{N} \\frac{1}{10^{i}}(x_i - i)^2  \r\n",
        "\\end{align}\r\n",
        "\r\n",
        "for $ \\textbf{N = 3}$,  we can express  **h(x)** as: \\\\\r\n",
        "\\begin{align}\r\n",
        "\\ h(x) = \\frac{1}{10^{1}}(x_1 - 1)^2  + \\frac{1}{10^{2}}(x_2 - 2)^2 + \\frac{1}{10^{3}}(x_3 - 3)^2 \r\n",
        "\\end{align}  \\\\\r\n",
        "\r\n",
        "which can be equivalently expressed in the form: \\\\\r\n",
        "\\begin{align}\r\n",
        "\\ h(x) = \\begin{bmatrix}x_{1} & x_{2} & x_{3}\r\n",
        "\\end{bmatrix}\\ \\begin{bmatrix} \\frac{1}{10} & 0 & 0 \\\\ 0 & \\frac{1}{100} & 0 \\\\ 0 & 0 & \\frac{1}{1000}\\end{bmatrix}\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3} \\end{bmatrix} + 2\\begin{bmatrix}-\\frac{1}{10} & -\\frac{2}{100} & -\\frac{3}{1000}\\end{bmatrix}\\begin{bmatrix}\r\n",
        "x_{1} \\\\ x_{2} \\\\ x_{3}\\end{bmatrix} + \\frac{149}{1000}  \r\n",
        "= \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c \\end{align} \\\\\r\n",
        "$ where ,\\ \\mathbf{x} = \\begin{bmatrix}x_{1} \\\\ x_{2} \\\\ x_{3} \r\n",
        "\\end{bmatrix},\r\n",
        "\\hspace{1.5cm} \\mathbf{A} = \\begin{bmatrix} 1/10 & 0 & 0 \\\\ 0 & 1/100 & 0 \\\\ 0 & 0 & 1/1000\\end{bmatrix} \\ , \\ \\ \\mathbf{b} = \\begin{bmatrix}-\\frac{1}{10} \\\\ -\\frac{2}{100} \\\\ -\\frac{3}{1000}\\end{bmatrix}\\  \\ , \\ c = \\  \\frac{149}{1000} $\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b7uoqb4bqR6"
      },
      "source": [
        "import numpy as np "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIN6xAXadL07"
      },
      "source": [
        "def evalf(x):  \r\n",
        "  #Input: x is a numpy array of size 3 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 3 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the objective function value\r\n",
        "  #compute the function value and return it \r\n",
        "  return (1/10)*((x[0]-1)**2) + (1/100)*((x[1]-2)**2) + (1/1000)*((x[2]-3)**2)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndGlq2p1dO0i"
      },
      "source": [
        "def evalg(x):  \r\n",
        "  #Input: x is a numpy array of size 3 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 3 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the gradient value\r\n",
        "  #compute the gradient value and return it \r\n",
        "  return np.array([0.2*((x[0]-1)), 0.02*((x[1]-2)), 0.002*((x[2]-3))])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2rBuslsdWVy"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\r\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 3 \r\n",
        "  assert type(A) is np.ndarray and A.shape[0] == 3 and  A.shape[1] == 3 #allow only a 3x3 array \r\n",
        "  #Complete the code to compute step length\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  step_length = np.matmul(gr_t, gradf)/(2*np.matmul(np.matmul(gr_t, A), gradf))\r\n",
        "  return step_length"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj5Wf0wQdaAB"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\r\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(x) == 3 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 3 \r\n",
        "  \r\n",
        "  alpha = alpha_start\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  #implement the backtracking line search\r\n",
        "  while evalf(np.add(x,-alpha*gradf)) > evalf(x)-gamma*alpha*np.matmul(gr_t, gradf):\r\n",
        "    alpha = rho*alpha\r\n",
        "  #print('final step length:',alpha)\r\n",
        "  return alpha"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxMmLcQSyJjk"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\r\n",
        "EXACT_LINE_SEARCH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0Dab4_DyQZB"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size 3, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 3 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  # construct a suitable A matrix for the quadratic function \r\n",
        "  A = np.array([[1/10, 0, 0],[0, 1/100, 0],[0,0,1/1000]])\r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "\r\n",
        "  #initialization for backtracking line search\r\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2]\r\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\r\n",
        "\r\n",
        "  k = 0\r\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "  \r\n",
        "    if line_search_type == EXACT_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 0.1\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    \r\n",
        "    #implement the gradient descent steps here   \r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x) #compute gradient at new point\r\n",
        "\r\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  return x , k , evalf(x)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH12bkCN9cN-"
      },
      "source": [
        "**2nd & 3rd ANSWER :** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jXxbqeq4ae6",
        "outputId": "2b0fa89e-2c64-4cc0-8b70-f91fce9d3d5f"
      },
      "source": [
        "# Let us assume that the starting-point and stopping-tolerance for 2nd question be as mentioned below: \r\n",
        "my_start_x = np.array([1/100, 1/10, 1])\r\n",
        "my_tol= 1e-9\r\n",
        "\r\n",
        "x_opt_els, k, f_val_els = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\r\n",
        "print(\"For Exact line search method:\")\r\n",
        "print(\"Minimizer = \",x_opt_els, \", iterations = \",k ,\", Minimum function value = \", f_val_els)\r\n",
        "\r\n",
        "x_opt_bls, k, f_val_bls = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\r\n",
        "print(\"\\nFor BACKTRACKING_LINE_SEARCH method:\")\r\n",
        "print(\"Minimizer = \",x_opt_bls, \", iterations = \",k ,\", Minimum function value = \", f_val_bls)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Exact line search method:\n",
            "Minimizer =  [1.         2.         2.99999952] , iterations =  245 , Minimum function value =  2.2763158970246047e-16\n",
            "\n",
            "For BACKTRACKING_LINE_SEARCH method:\n",
            "Minimizer =  [1.        2.        2.9999995] , iterations =  7594 , Minimum function value =  2.4929930196397745e-16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMvvhynAJ3Hf"
      },
      "source": [
        "**Comment for 3rd question :** \\\\\r\n",
        "Here, the number of **iterations** required to achieve the minimizer and the minimum value of the function, by using the Exact step length procedure is comparatively very small (i.e. 245) to that of the backtracking line search procedure (i.e. 7594). Also minimizer remains same in both cases. However, there is a little difference in the function value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snAaOJz2Mjnj"
      },
      "source": [
        "**4 SOLUTION :** \\\\\r\n",
        "$For, \\ \\  N = 4$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzVFFwQK6OeZ"
      },
      "source": [
        "def evalf(x):  \r\n",
        "  #Input: x is a numpy array of size 4 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 4 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the objective function value\r\n",
        "  #compute the function value and return it \r\n",
        "  return (1/10)*((x[0]-1)**2) + (1/100)*((x[1]-2)**2) + (1/1000)*((x[2]-3)**2) + (1/10000)*((x[3]-4)**2)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3hPH8-fNMBt"
      },
      "source": [
        "def evalg(x):  \r\n",
        "  #Input: x is a numpy array of size 4 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 4 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the gradient value\r\n",
        "  #compute the gradient value and return it \r\n",
        "  return np.array([0.2*(x[0]-1), 0.02*(x[1]-2), 0.002*(x[2]-3), 0.0002*(x[3]-4)])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPmLNOajNXW6"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\r\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 4 \r\n",
        "  assert type(A) is np.ndarray and A.shape[0] == 4 and  A.shape[1] == 4 #allow only a 4x4 array \r\n",
        "  #Complete the code to compute step length\r\n",
        "  #A = np.identity(4)\r\n",
        "  #gradf = evalg(x)\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  step_length = np.matmul(gr_t, gradf)/(2*np.matmul(np.matmul(gr_t, A), gradf))\r\n",
        "  return step_length"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQ3Af6DNaw_"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\r\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(x) == 4 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 4 \r\n",
        "  \r\n",
        "  alpha = alpha_start\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  #implement the backtracking line search\r\n",
        "  while evalf(np.add(x,-alpha*gradf)) > evalf(x)-gamma*alpha*np.matmul(gr_t, gradf):\r\n",
        "    alpha = rho*alpha\r\n",
        "  #print('final step length:',alpha)\r\n",
        "  return alpha"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTUuJf6wNwD6"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\r\n",
        "EXACT_LINE_SEARCH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koaXHwrVN0r_"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size 3, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 4 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  # construct a suitable A matrix for the quadratic function \r\n",
        "  A = np.array([[1/10, 0, 0,0],[0, 1/100, 0,0],[0,0,1/1000,0],[0,0,0,1/10000]])\r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "\r\n",
        "  #initialization for backtracking line search\r\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2]\r\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\r\n",
        "\r\n",
        "  k = 0\r\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "  \r\n",
        "    if line_search_type == EXACT_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 0.1\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    \r\n",
        "    #implement the gradient descent steps here   \r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x) #compute gradient at new point\r\n",
        "\r\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  return x , k , evalf(x)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvlQ1NjKTCO9",
        "outputId": "e2decc0c-bd71-4265-b1b6-dec01baa7e28"
      },
      "source": [
        "my_start_x = np.array([1/1000,1/100, 1/10, 1])\r\n",
        "my_tol= 1e-9\r\n",
        "\r\n",
        "x_els, k, fval_els = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\r\n",
        "print(\"For Exact line search method:\")\r\n",
        "print(\"Minimizer = \",x_els, \", iterations = \",k ,\", Minimum function value = \", fval_els)\r\n",
        "\r\n",
        "x_bls, k, fval_bls = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\r\n",
        "print(\"\\nFor BACKTRACKING_LINE_SEARCH method:\")\r\n",
        "print(\"Minimizer = \",x_bls, \", iterations = \",k ,\", Minimum function value = \", fval_bls)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Exact line search method:\n",
            "Minimizer =  [1.         2.         3.         3.99999527] , iterations =  2249 , Minimum function value =  2.2403065892237764e-15\n",
            "\n",
            "For BACKTRACKING_LINE_SEARCH method:\n",
            "Minimizer =  [1.       2.       3.       3.999995] , iterations =  66517 , Minimum function value =  2.4997720868765857e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JDa-VoUp8G"
      },
      "source": [
        "**Comment :** \\\\\r\n",
        "here we have a similiar observation as we have found in the 3rd question which is : \\\\\r\n",
        "**1.** the number of iterations required to achieve the minimizer and the minimum value of the function, by using the Exact step length procedure is comparatively very small (i.e. 2249) to that of the backtracking line search procedure (i.e. 66517). \\\\\r\n",
        "**2.** Also minimizer remains same in both cases. However, there is a little difference in the function value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGgyEoal7eTV"
      },
      "source": [
        "**5. ANSWER :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrByzSbgA6P"
      },
      "source": [
        "**Comment on the possible observations for N > 4 :** \\\\\r\n",
        "1. Number of iterations in case of exact line search will be less than the backtracking line search procedure. Also as the $N$ value increases the number of iterations in both cases also increases. \\\\\r\n",
        "2. Minimizer will remain same in both the cases. \\\\\r\n",
        "3. The difference in the function value for both cases will increase for increasing $N$ value."
      ]
    }
  ]
}