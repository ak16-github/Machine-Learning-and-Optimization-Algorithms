{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190011_IE 684_Lab 2_Ex 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 2. Exercise 1. }$\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "Recall that we implemented the gradient descent algorithm to solve $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$. The main ingredients in the gradient descent iterations are the descent direction $\\mathbf{p}^k$ which is set to $-\\nabla f(\\mathbf{x}^k)$, and the step length $\\eta^k$ which is found by solving an optimization problem (or sometimes taken as a constant value over all iterations). We used the following procedure in the previous lab:\r\n",
        "\r\n",
        "\\begin{align}\r\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\r\n",
        "& \\textbf{Initialize } k=0 \\\\ \r\n",
        "& \\mathbf{p}^k =-\\nabla f(\\mathbf{x}^k) \\\\ \r\n",
        "&\\textbf{While } \\| \\mathbf{p}^k \\|_2 > \\tau \\text{ do:}  \\\\   \r\n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k + \\eta  \\mathbf{p}^k) = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\r\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} = \\mathbf{x}^k + \\eta^k \\mathbf{p}^k = \\mathbf{x}^k - \\eta^k \\nabla f (\\mathbf{x}^k)  \\\\ \r\n",
        "&\\quad \\quad k = {k+1} \\\\ \r\n",
        "&\\textbf{End While} \\\\\r\n",
        "&\\textbf{Output: } \\mathbf{x}^k\r\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "We saw that for quadratic functions, a closed form analytical solution for the minimizer of the optimization problem $\\min_{\\eta \\geq 0} f({\\mathbf{x}}^k + \\eta {\\mathbf{p}}^k)$ exists. However finding a closed form expression as a solution to this optimization problem to find a suitable step length might not always be possible. To tackle general situations, we will try to devise a different procedure in this lab. \r\n",
        "\r\n",
        "To find the step length, we will use the following property: \r\n",
        "Suppose a non-zero $\\mathbf{p} \\in {\\mathbb{R}}^n$ is a descent direction at point $\\mathbf{x}$, and let $\\gamma \\in (0,1)$. Then there exists $\\varepsilon >0$ such that  \r\n",
        "\\begin{align}\r\n",
        "f(\\mathbf{x}+\\alpha \\mathbf{p}) \\leq f(\\mathbf{x}) + \\gamma \\alpha \\nabla f(\\mathbf{x})^\\top \\mathbf{p}, \\ \\forall \\alpha \\in (0,\\varepsilon].  \r\n",
        "\\end{align}\r\n",
        "\r\n",
        "This condition is called a $\\textbf{sufficient decrease condition}$. \\\\\r\n",
        "The step length $\\eta^k$ can be found using a backtracking procedure illustrated below to find appropriate value of $\\varepsilon$.  \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6OddaNAmpA"
      },
      "source": [
        "\r\n",
        "\\begin{align}\r\n",
        "& \\textbf{Input:}  \\text{ $\\mathbf{x}^k$, $\\mathbf{p}^k$, $\\alpha^0$, $\\rho \\in (0,1)$, $\\gamma \\in (0,1)$ }  \\\\\r\n",
        "& \\textbf{Initialize } \\alpha=\\alpha^0 \\\\ \r\n",
        "&\\textbf{While } f(\\mathbf{x}^k + \\alpha \\mathbf{p}^k)   > f(\\mathbf{x}^k) + \\gamma \\alpha \\nabla f(\\mathbf{x}^k)^\\top \\mathbf{p}^k \\text{ do:}  \\\\   \r\n",
        "&\\quad \\quad \\alpha = \\rho \\alpha  \\\\\r\n",
        "&\\textbf{End While} \\\\\r\n",
        "&\\textbf{Output: } \\alpha\r\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-xcDISWmGR"
      },
      "source": [
        "In this exercise, we will check if finding the steplength using the backtracking procedure is advantageous for some quadratic functions. In this sample code we consider $f(\\mathbf{x})=f(x_1,x_2) = (x_1-2)^2 + (x_2 + 3)^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/1.19/ for numpy documentation\r\n",
        "#we will first import the numpy package and name it as np\r\n",
        "import numpy as np \r\n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \r\n",
        "def evalf(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the objective function value\r\n",
        "  #compute the function value and return it \r\n",
        "  return (x[1]+3)**2 + (-2+x[0])**2\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \r\n",
        "def evalg(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the gradient value\r\n",
        "  #compute the gradient value and return it \r\n",
        "  return np.array([2*(x[0]-2), 2*(x[1]+3)])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\r\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(A) is np.ndarray and A.shape[0] == 2 and  A.shape[1] == 2 #allow only a 2x2 array \r\n",
        "  #Complete the code to compute step length\r\n",
        "  #A = np.identity(2)    #we have already taken A as np.array([[1, 0],[0,1]]) in the lower cell.\r\n",
        "  #gradf = evalg(x)\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  step_length = np.matmul(gr_t, gradf)/(2*np.matmul(np.matmul(gr_t, A), gradf))\r\n",
        "  return step_length"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\r\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  \r\n",
        "  alpha = alpha_start\r\n",
        "  gr_t = np.matrix.transpose(gradf)\r\n",
        "  #implement the backtracking line search\r\n",
        "  while evalf(np.add(x,-alpha*gradf)) > evalf(x)-gamma*alpha*np.matmul(gr_t, gradf):\r\n",
        "    alpha = rho*alpha\r\n",
        "  #print('final step length:',alpha)\r\n",
        "  return alpha"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUUdzLtVSCl"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\r\n",
        "EXACT_LINE_SEARCH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  # construct a suitable A matrix for the quadratic function \r\n",
        "  A = np.array([[1, 0],[0,1]])\r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "\r\n",
        "  #initialization for backtracking line search\r\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2]\r\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\r\n",
        "\r\n",
        "  k = 0\r\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "  \r\n",
        "    if line_search_type == EXACT_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 0.1\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    \r\n",
        "    #implement the gradient descent steps here   \r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x) #compute gradient at new point\r\n",
        "\r\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  return x , k , evalf(x)\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIQTRJbq05wj"
      },
      "source": [
        "**3 ANSWER:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada6a662-4fd0-4aa1-e4b5-1693d87ee8ca"
      },
      "source": [
        "\r\n",
        "my_start_x = np.array([5,5])\r\n",
        "my_tol= 1e-5\r\n",
        "\r\n",
        "print(\"For CONSTANT_STEP_LENGTH procedure :\")\r\n",
        "x_opt, k, f_value = find_minimizer(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\r\n",
        "print(\"Minimizer = \",x_opt,\",Iteration = \",k,\", Minimum function value = \", f_value) \r\n",
        "\r\n",
        "print(\"\\nFor EXACT_LINE_SEARCH procedure :\")\r\n",
        "x_opt_els, k, f_value = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\r\n",
        "print(\"Minimizer = \",x_opt_els,\",Iteration = \", k,\", Minimum function value = \", f_value)\r\n",
        "\r\n",
        "print(\"\\nFor BACKTRACKING_LINE_SEARCH procedure :\")\r\n",
        "x_opt_bls, k, f_value = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\r\n",
        "print(\"Minimizer = \",x_opt_bls,\",Iteration = \", k , \", Minimum function value = \",f_value)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For CONSTANT_STEP_LENGTH procedure :\n",
            "Minimizer =  [ 2.00000151 -2.99999598] ,Iteration =  65 , Minimum function value =  1.8408617293418483e-11\n",
            "\n",
            "For EXACT_LINE_SEARCH procedure :\n",
            "Minimizer =  [ 2. -3.] ,Iteration =  1 , Minimum function value =  0.0\n",
            "\n",
            "For BACKTRACKING_LINE_SEARCH procedure :\n",
            "Minimizer =  [ 2. -3.] ,Iteration =  1 , Minimum function value =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIAc53JV1J_Y"
      },
      "source": [
        "**4 SOLUTION:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieARv4kn1Rud",
        "outputId": "c69a2cb8-7682-479c-a438-fc3d64886254"
      },
      "source": [
        "my_start_x = np.array([10,10])\r\n",
        "my_tol= 1e-12\r\n",
        "\r\n",
        "print(\"For Exact line search:\")\r\n",
        "x_opt_els, k, f_value = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\r\n",
        "print(\"Minimizer = \",x_opt_els,\",Iterations = \", k,\",f(x) = \", f_value)\r\n",
        "\r\n",
        "print(\"\\nFor Backtracking_line_search:\")\r\n",
        "x_opt_bls, k, f_value = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\r\n",
        "print(\"Minimizer = \",x_opt_bls,\",Iterations = \", k ,\",f(x) = \", f_value)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Exact line search:\n",
            "Minimizer =  [ 2. -3.] ,Iterations =  1 ,f(x) =  0.0\n",
            "\n",
            "For Backtracking_line_search:\n",
            "Minimizer =  [ 2. -3.] ,Iterations =  1 ,f(x) =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuNwgE2A9Fqf"
      },
      "source": [
        "**Comment :** \\\\\r\n",
        "Here, from the above result we can say that the number of iterations required to obtain the minimizer is same (equal to 1) for both the exact line search and the backtracking line search procedure. Also the minimizer and the corresponding function value is same in both cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt_7PxGG_vnA"
      },
      "source": [
        "**5 SOLUTION:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8ItL-zU1SOg",
        "outputId": "e8f564df-0832-4cca-9a7c-26cdaaeca68a"
      },
      "source": [
        "# Backtracking line search for different values of alpha\r\n",
        "my_start_x = np.array([10,10])\r\n",
        "my_tol= 1e-9\r\n",
        "alpha_list = [1, 0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01]\r\n",
        "iterations_bls = []\r\n",
        "\r\n",
        "for alpha_start in alpha_list:\r\n",
        "  x_opt_bls, k, f_value = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, 0.5, 0.5)\r\n",
        "  iterations_bls.append(k)\r\n",
        "  print('for alpha = ',alpha_start, ', iter:', k, ', x: ',x_opt_bls, ', f(x): ', f_value)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for alpha =  1 , iter: 1 , x:  [ 2. -3.] , f(x):  0.0\n",
            "for alpha =  0.9 , iter: 11 , x:  [ 2. -3.] , f(x):  2.3300003855701448e-20\n",
            "for alpha =  0.75 , iter: 18 , x:  [ 2. -3.] , f(x):  4.9339669177562995e-20\n",
            "for alpha =  0.6 , iter: 27 , x:  [ 2. -3.] , f(x):  7.561282499767123e-20\n",
            "for alpha =  0.5 , iter: 1 , x:  [ 2. -3.] , f(x):  0.0\n",
            "for alpha =  0.4 , iter: 16 , x:  [ 2. -3.] , f(x):  1.0007255782441984e-20\n",
            "for alpha =  0.25 , iter: 35 , x:  [ 2. -3.] , f(x):  1.9735867671025198e-19\n",
            "for alpha =  0.1 , iter: 109 , x:  [ 2. -3.] , f(x):  1.7417011453747528e-19\n",
            "for alpha =  0.01 , iter: 1195 , x:  [ 2. -3.] , f(x):  2.498486159829666e-19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C-kGUJ31SmZ",
        "outputId": "981cf9c3-0a9b-467a-d425-1f7b74294206"
      },
      "source": [
        "iterations_bls"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 11, 18, 27, 1, 16, 35, 109, 1195]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6bdd41ef-e6c3-40ba-98f9-00612352f53e"
      },
      "source": [
        "#Plotting Iterations against alpha values\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#plt.figure(figsize=(width,height))\r\n",
        "plt.plot(alpha_list, iterations_bls)\r\n",
        "plt.xlabel('alpha values',color='b')\r\n",
        "plt.ylabel('Iterations',color='b')\r\n",
        "plt.title('Iterations against alpha',color='r')\r\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xcdX3/8dd7r0l2c90dAiSBZJcAImrFiKC2tqKI1IL+iorWCkpLfxbUSmvF2t9Pf+1Pq9XW4o96QaHipYq31hQpFhGlIKDhqtxkCYSEWzabC0k2yd4+vz/OdzeTZXdnNtmZycy8n4/HPmbme86Z8zm7yb73XOZzFBGYmZlNpaHSBZiZ2cHPYWFmZgU5LMzMrCCHhZmZFeSwMDOzghwWZmZWkMPCape0A6mr0mXMCOlepN+ucA3nIt004/NaVXBYWGlIjyK9Kj0v/S8O6SdIf7TPWEQ7EWtLut5yiXguET85oPeQPoL0tZkpyOqNw8IOflJTpUswq3cOCyst6TnA54GT02GhrWm8FelTSI8hPY30eaTZadpvI21A+gDSU8C/IC1EuhqpF2lLer40zf9R4DeBS9M6Lk3jgXRUej4f6Stp+XVIf43UkKZlez5ZPVuQHkF6bd42nIu0Fml7mvYHk2zriUi3IG1FehLpUqSWvOmnIj2ItA3ps0g/HdsbkrqRfozUh7QJ6etIC/KWzd9T+wjSt9L2bE+HqFblzfsBpMfTtAeRTkE6Dfgr4M3pe3T3JNtwMdLDadn7kN4wxc82kN6TvjebkD459j3dO89k39N3IN2f1rMW6U8mXY8dFBwWVloR9wP/E7glHRYa/QX4ceBo4DeAo4AlwP/OW/JQYBFwJHA+2b/Vf0mvjwB2AZemdXwI+G/gwrSOCyeo5P8B84Eu4BXA24F35E1/CfAg0An8PXA5kpDagM8AryViLvBS4K5JtnYYeF96j5OBU4A/BUDqBL4DfBDoSOt6ad6yAv4OOBx4DrAM+Mgk6wE4A/gmsABYPfa9kI4BLgRenOp9DfAoEdcCHwOuSt+jF0zyvg+TBe984P8AX0M6bIo63gCsAk4AzgTemTdt4u9pZiPwOmAe2c/h00gnTLEeqzCHhZVf9gvjfOB9RGwmYjvZL7Kz8+YaAT5MxB4idhHRR8R3iehP83+U7Jd+MetrTO/9QSK2E/Eo8A/AH+bNtY6ILxIxDFwJHAYszqvleKTZRDxJxL0TrifidiJuJWIoreMLeTWeDtxLxPeIGCILoKfylu0h4rq0vb3APxbYvpuIuCbV+1Vg9Jf/MNAKHIfUTMSjRDxc+Js0Vse3iXiCiBEirgIeAk6cYolPpJ/hY8A/AW/Jmzb59zTiB0Q8TEQQ8VPgv8hCyg5SDgurhBwwB7g9HbLZClybxkf1ErF77JU0B+kL6RDSM8CNwIIUBIV0As3AuryxdWR7M6Pyf3H3p2ftROwE3ky2d/Qk0g+Qjp1wLdLR6fDYU6nGj6V1Q7bHsD5vHQFsyFt2MdI30+GjZ4Cv5S07kafynvcDs5CaiOgB/oxsr2Rjes/Dp3if8dvwdqS78n4uxxeoY33e83Vk2/nsGvO/p9l6Xot0K9LmtJ7TC6zHKsxhYeUwvrXxJrLDSM8lYkH6mk9E+xTL/DlwDPASIuYBv5XGNcn849c3SHYIa9QRwOPFVR8/JOLVZH8ZPwB8cZI5P5emr0w1/lVefU8CS8fmzPauluYt+7G0Dc9Ly74tb9npifhXIl5Otr0BfGJ0ypTLSUeSbduFQEc6ZPirAnUsy3t+BPBEwfqkVuC7wKeAxWk91xRYj1WYw8LK4Wlg6djJ3ogRsl9Kn0Y6BABpCdJrpniPuWQBsxVpEfDhCdYx8WcqssMg3wI+ijQ3/VK8iOyv96llf/Gfmc5d7AF2kB2WmqzGZ4Adae/jXXnTfgA8D+n1ZFd3XUB2XiZ/2R3ANqQlwPsL1jZxvccgvTL9Qt5N9j0brfdpYPmzTkLv1UYWKL3pvd5BtmcxlfeTXXywDHgvcFURVbaQHSrrBYbSie9Ti1jOKshhYeXwY+Be4CmkTWnsA0APcGs67PIjsj2HyfwTMJtsL+FWssNW+S4BzkpX3nxmguXfDewE1gI3Af8KXFFE7Q1kwfIEsJnsPMK7Jpn3L4C3AtvJwnDvL86ITcAbyU709gHHAWvIAgiyk8knANvIguV7RdQ2kVayiwc2kR0GOoTspDrAt9NjH9Idz1oy4j6yczm3kAXL84CbC6zv+8DtZCf9fwBcXrDC7JzTe8gCfAvZ92x1weWsouSbH5lVQPbX/QbgD4i4odLl7BcpyA659VS6FCs971mYlYv0GqQF6RDR6PmMWytclVlRHBZm5XMy2ecYNgG/B7yeiF2VLcmsOD4MZWZmBXnPwszMCqrJBm2dnZ2xfPnySpdhZlZVbr/99k0RkZtoWk2GxfLly1mzZk2lyzAzqyqS1k02zYehzMysIIeFmZkV5LAwM7OCHBZmZlZQycJC0hWSNkr6Vd7YJyU9IOkeSf+mvDuBSfqgpB5JDyqvoZyk09JYj6SLS1WvmZlNrpR7Fl8GThs3dh1wfEQ8H/g1qcGZpOPIbk7z3LTMZyU1KrtXwT8DryVrvPaWNK+ZmZVRycIiIm4k69KZP/Zfkd0lDLKeOKP9/M8EvhkReyLiEbJupCemr56IWBsRA2S3kTyzVDWbmdnEKnnO4p3Af6bnS9j3jlsb0thk488i6XxJaySt6e3t3a+CtvUPcsmPHuLu9Vv3a3kzs1pVkbCQ9CFgCPj6TL1nRFwWEasiYlUuN+EHEAtqaIBP/+jX3PzwpsIzm5nVkbJ/glvSucDrgFNibxfDx9n39oxL2XvLy8nGZ9zcWc3k5raytndnqVZhZlaVyrpnIek04C+BM2LvDdwhu0vW2ZJaJa0AVgI/B34BrJS0QtktOc+mxHfU6s61sbZ3RylXYWZWdUp56ew3yG7PeIykDZLOAy4lu9fwdZLukvR5gIi4l+wWi/eR3S7zgogYTifDLwR+CNwPfCvNWzJduXYe7t2JW7ebme1VssNQEfGWCYYnvT9vRHwU+OgE49cA18xgaVPqzrWzbdcgm3cO0NHeWq7Vmpkd1PwJ7nG6cm0APOzzFmZmYxwW43R3tgP4vIWZWR6HxThLFs6mpamBtZu8Z2FmNsphMU5jg1jR0cbDG71nYWY2ymExge5D2rxnYWaWx2Exga7Odh7b3M/A0EilSzEzOyg4LCbQlWtjeCR4bLP3LszMwGExoe5cdkWUL581M8s4LCaw97MWPsltZgYOiwnNndXMIW4oaGY2xmExia5cm/cszMwSh8UkunLtrHVDQTMzwGExqfyGgmZm9c5hMQk3FDQz28thMYmjcm4oaGY2ymExicMXZA0FfZLbzMxhManRhoK+fNbMzGExJTcUNDPLOCym4IaCZmYZh8UUug9xQ0EzM3BYTKkr3WK1Z6PDwszqm8NiCqOftVi7yVdEmVl9c1hMwQ0FzcwyDosC3FDQzKyEYSHpCkkbJf0qb2yRpOskPZQeF6ZxSfqMpB5J90g6IW+Zc9L8D0k6p1T1TqbbDQXNzEq6Z/Fl4LRxYxcD10fESuD69BrgtcDK9HU+8DnIwgX4MPAS4ETgw6MBUy5dbihoZla6sIiIG4HN44bPBK5Mz68EXp83/pXI3AoskHQY8BrguojYHBFbgOt4dgCVlBsKmpmV/5zF4oh4Mj1/Clicni8B1ufNtyGNTTZeNm4oaGZWwRPckZ0EmLETAZLOl7RG0pre3t6Zels3FDQzo/xh8XQ6vER63JjGHweW5c23NI1NNv4sEXFZRKyKiFW5XG7GCm5sEF2dbihoZvWt3GGxGhi9oukc4Pt5429PV0WdBGxLh6t+CJwqaWE6sX1qGiurrpwbCppZfSvlpbPfAG4BjpG0QdJ5wMeBV0t6CHhVeg1wDbAW6AG+CPwpQERsBv4W+EX6+ps0VlZuKGhm9a6pVG8cEW+ZZNIpE8wbwAWTvM8VwBUzWNq05TcUPOqQuZUsxcysIvwJ7iK4oaCZ1TuHRRHcUNDM6p3DoghuKGhm9c5hUSQ3FDSzeuawKJIbCppZPXNYFGm0oWCfGwqaWR1yWBSpe/Qkt89bmFkdclgUqdsNBc2sjjksiuSGgmZWzxwWRXJDQTOrZw6LafDls2ZWrxwW09Cda2f9ll1uKGhmdcdhMQ1dub0NBc3M6onDYhrcUNDM6pXDYhrcUNDM6pXDYhpGGwo+7D0LM6szDotp6s61e8/CzOqOw2KaunJtbihoZnXHYTFNbihoZvXIYTFNbihoZvXIYTFNow0F/UluM6snDotpOnzBbFqbGtx91szqisNimhobxAo3FDSzOuOw2A9uKGhm9aYiYSHpfZLulfQrSd+QNEvSCkm3SeqRdJWkljRva3rdk6Yvr0TN+UYbCu4ZGq50KWZmZVH2sJC0BHgPsCoijgcagbOBTwCfjoijgC3AeWmR84AtafzTab6KGmso2Ndf6VLMzMqiUoehmoDZkpqAOcCTwCuB76TpVwKvT8/PTK9J00+RpDLW+ix7r4jyeQszqw9lD4uIeBz4FPAYWUhsA24HtkbEUJptA7AkPV8CrE/LDqX5O8pZ83grOt1Q0MzqSyUOQy0k21tYARwOtAGnzcD7ni9pjaQ1vb29B/p2U3JDQTOrN5U4DPUq4JGI6I2IQeB7wMuABemwFMBS4PH0/HFgGUCaPh/oG/+mEXFZRKyKiFW5XK7U2+CGgmZWVyoRFo8BJ0mak849nALcB9wAnJXmOQf4fnq+Or0mTf9xHARd/NxQ0MzqSSXOWdxGdqL6DuCXqYbLgA8AF0nqITsncXla5HKgI41fBFxc7pon0u2GgmZWR5oKzzLzIuLDwIfHDa8FTpxg3t3AG8tR13R05TUU7GxvrXA1Zmal5U9w7yc3FDSzeuKw2E9uKGhm9cRhsZ/cUNDM6onD4gB059p9GMrM6oLD4gB05drcUNDM6oLD4gC4oaCZ1QuHxQFwQ0EzqxcOiwPghoJmVi+KCguJv5eYJ9Escb1Er8TbSl3cwW7urGYWz3NDQTOrfcXuWZwawTPA64BHgaOA95eqqGrS1emGgmZW+4oNi9G2IL8LfDuCbSWqp+p05dp4eOMONxQ0s5pWbFhcLfEA8CLgeokcsLt0ZVWP7lw7z+weckNBM6tpRYVFBBcDLwVWRTAI7CS7gVHdy28oaGZWq6bTdfZYYLm0zzJfmeF6qk5+Q8ETVyyqcDVmZqVRVFhIfBXoBu4CRj+uHDgsWOKGgmZWB4rds1gFHBeBz+KO05AaCvqDeWZWy4o9wf0r4NBSFlLNunPt3rMws5pW7J5FJ3CfxM+BPaODEZxRkqqqTFeujWvvfYo9Q8O0NjVWuhwzsxlXbFh8pJRFVLvuXPtYQ8GVi+dWuhwzsxlX7KWzPwUeAOamr/vTmLH38lmftzCzWlVsb6g3AT8H3gi8CbhN4qxSFlZNRhsK+kZIZlarij0M9SHgxRFsBEif4P4R8J1SFVZNRhsK+oN5Zlarir0aqmE0KJK+aSxbF9xQ0MxqWbF7FtdK/BD4Rnr9ZuCa0pRUnboPaWP1XU8QEUiqdDlmZjOqqLCI4P0Svw+8LA1dFsG/la6s6tPVubehYGd7a6XLMTObUUX3horgu8B3Z2KlkhYAXwKOJ2sb8k7gQeAqYDnZPTPeFBFblP2ZfglwOtAPnBsRd8xEHTNp7IqojTscFmZWc6Y87yBxU3rcLvFM3td2iWcOYL2XANdGxLHAC4D7gYuB6yNiJXB9eg3wWmBl+jof+NwBrLdkRhsKrt3kk9xmVnumDIsIXp4e50YwL+9rbgTz9meFkuYDvwVcnr13DETEVrKW51em2a4EXp+enwl8JTK3AgskHbY/6y4lNxQ0s1pW7OcsvlrMWJFWAL3Av0i6U9KXJLUBiyPiyTTPU8Di9HwJsD5v+Q1pbFw9Ol/SGklrent797O0/eeGgmZWy4q9/PW5+S/SPS1etJ/rbAJOAD4XES8ku5HSxfkzRHaP0ml1uI2IyyJiVUSsyuVy+1nagXFDQTOrVYXOWXxQYjvw/PzzFcDTwPf3c50bgA0RcVt6/R2y8Hh69PBSehz9XMfjwLK85ZemsYNOV66N9Vt2sWdouPDMZmZVpNA5i7+LYC7wyXHnKzoi+OD+rDAingLWSzomDZ0C3AesBs5JY+ewN4xWA29X5iRgW97hqoNKfkNBM7NaUuznLD4osZDsiqRZeeM37ud63w18XVILsBZ4B1lwfUvSecA6sh5UkH3473Sgh+zS2Xfs5zpLLr+hoLvPmlktKfa2qn8EvJfsENBdwEnALcAr92elEXEX2d33xjtlgnkDuGB/1lNuXXn34zYzqyXFnuB+L/BiYF0EvwO8ENhasqqqVHtrkxsKmllNKjYsdkewG0CiNYIHgGMKLFOX3FDQzGpRsWGxQWIB8O/AdRLfJzuvYON0H9LGwxt3kB09MzOrDcWe4H5DevoRiRuA+cC1JauqirmhoJnVooJhIdEI3BvBsTB2i1WbRPch6SS3GwqaWQ0peBgqgmHgQYkjylBP1etKt1h1Q0EzqyXFtihfCNwr8XOy9hwARHBGSaqqYm4oaGa1qNiw+F8lraKGuKGgmdWiYk9w/1TiSGBlBD+SmAM0lra06tWda+feJ7ZVugwzsxlTbIvyPyZr+PeFNLSE7DJam0B3ro3HNve7oaCZ1YxiP2dxAdn9t58BiOAh4JBSFVXtunLtjARuKGhmNaPYsNgTwcDoi3Q/C3/qbBL5DQXNzGpBsWHxU4m/AmZLvBr4NvAfpSururmhoJnVmmLD4mKyW6H+EvgT4JoIPlSyqqqcGwqaWa0p9tLZd0dwCfDF0QGJ96Yxm0B3rt17FmZWM4rdszhngrFzZ7COmtOVa2NtrxsKmlltmHLPQuItwFuBFRKr8ybNBTaXsrBq54aCZlZLCh2G+hnwJNAJ/EPe+HbgnlIVVQvcUNDMasmUYRHBOrL7VpxcnnJqR35DwZd0dVS4GjOzA1PoMNR2Jv48hYCIYF5JqqoBow0FH97ok9xmVv0K7VnMLVchtWa0oaBblZtZLSj2aijbD925drcqN7Oa4LAoITcUNLNa4bAoITcUNLNaUbGwkNQo6U5JV6fXKyTdJqlH0lWSWtJ4a3rdk6Yvr1TN09XtHlFmViMquWfxXuD+vNefAD4dEUcBW4Dz0vh5wJY0/uk0X1VY4e6zZlYjKhIWkpYCvwt8Kb0W8EqyGywBXAm8Pj0/M70mTT8lzX/Qc0NBM6sVldqz+CfgL4GR9LoD2BoRQ+n1BrK78ZEe1wOk6dvS/PuQdL6kNZLW9Pb2lrL2aXFDQTOrBWUPC0mvAzZGxO0z+b4RcVlErIqIVblcbibf+oC4oaCZ1YJiW5TPpJcBZ0g6HZgFzAMuARZIakp7D0uBx9P8jwPLgA2SmoD5QF/5y94/3bmsoeCmHQPk5rpHlJlVp7LvWUTEByNiaUQsB84GfhwRfwDcAJyVZjsH+H56vpq9LdLPSvNXzZ/po3fN84fzzKyaHUyfs/gAcJGkHrJzEpen8cuBjjR+Edld+6pGfkNBM7NqVYnDUGMi4ifAT9LztcCJE8yzG3hjWQubQW4oaGa14GDas6hJbihoZrXAYVEG3Ye4oaCZVTeHRRl0d7qhoJlVN4dFGbihoJlVO4dFGbihoJlVO4dFGbihoJlVO4dFGbS3NnHovFluKGhmVcthUSZduTYfhjKzquWwKBM3FDSzauawKJP8hoJmZtXGYVEmbihoZtXMYVEm3Tk3FDSz6uWwKJPD589mVrMbCppZdXJYlElDg1je4YaCZladHBZl1H2I78dtZtXJYVFG3Z1trHdDQTOrQg6LMuo+xA0Fzaw6OSzKqKvTDQXNrDo5LMrIDQXNrFo5LMpotKGg9yzMrNo4LMos6xHlPQszqy4OizLrzrW7oaCZVR2HRZl15drcUNDMqo7DoszcUNDMqlHZw0LSMkk3SLpP0r2S3pvGF0m6TtJD6XFhGpekz0jqkXSPpBPKXfNM6vYVUWZWhSqxZzEE/HlEHAecBFwg6TjgYuD6iFgJXJ9eA7wWWJm+zgc+V/6SZ85oQ0HvWZhZNSl7WETEkxFxR3q+HbgfWAKcCVyZZrsSeH16fibwlcjcCiyQdFiZy54xDQ1iRWe7GwqaWVWp6DkLScuBFwK3AYsj4sk06SlgcXq+BFift9iGNDb+vc6XtEbSmt7e3pLVPBN8P24zqzYVCwtJ7cB3gT+LiGfyp0V2Xem0ri2NiMsiYlVErMrlcjNY6cxzQ0EzqzYVCQtJzWRB8fWI+F4afnr08FJ63JjGHweW5S2+NI1VrdGGguvcUNDMqkQlroYScDlwf0T8Y96k1cA56fk5wPfzxt+eroo6CdiWd7iqKo02FPRJbjOrFk0VWOfLgD8EfinprjT2V8DHgW9JOg9YB7wpTbsGOB3oAfqBd5S33JnX5ctnzazKlD0sIuImQJNMPmWC+QO4oKRFlVmbGwqaWZXxJ7grxA0FzayaOCwqpDuX3Y/bDQXNrBo4LCqkK9fGdjcUNLMq4bCokG43FDSzKuKwqBBfEWVm1cRhUSFuKGhm1cRhUSFuKGhm1cRhUUFuKGhm1cJhUUGjDQWvuOkRfv30dl9Ga2YHrUq0+7Dk1ccdyuq7n+Bvrr4PgNzcVl5+VCcvO6qTlx/VyaHzZ1W4QjOzjGrxr9lVq1bFmjVrKl1G0dZv7udnD2/ipp4+ftazib6d2WcvunNtY+FxUncH82Y1V7hSM6tlkm6PiFUTTnNYHFxGRoIHntrOzT2buKlnEz9/ZDO7BodpbBDPXzp/LDxeeMQCWpsaK12umdUQh0UV2zM0zJ2PbR0Lj7vXb2UkYHZzIyeuWDQWHsceOpeGhsn6M5qZFeawqCHbdg1y29q+sfAY/VBfR1sLLz2qk5cf1cHLjupk6cI5Fa7UzKrNVGHhE9xVZv7sZk597qGc+txDAXhy2y5u7tkbHv9x9xMALO+YM3ai/OTuDhbMaalk2WZW5bxnUUMigoc27uCmhzZxc88mbl3bx86BYSR43pL5Y+HxoiMXMqvZ5zvMbF8+DFWnBodHuHv9Vm7qycLjzse2MjQStDY18OLli8bC47jD59Ho8x1mdc9hYQDs2DPEzx/p46aHssNWDz69HYAFc5p5aXfHWHgcsWgO2a3Szaye+JyFAdDe2sQrj13MK49dDMDGZ3bzs4f7xvY8rvnlUwAsXTh77Cqrl3Z30NHeWsmyzewg4D0LA7LzHWs37cxOlD+0iVvW9rF99xAAxx02j5evzMLjxOWLmN3i8x1mtciHoWzahoZH+OXj28ausrp93RYGh4OWxgZOOHIBL1nRweJ5s1gwp5n5s/d+LZjTTHtrkw9jmVUhh4UdsP6BIX7x6JaxPY/7nnxm0nkbG7RPgOQHyb6vW5417qu0Zt62/kGe2T1Ia3MDrU2NzGpuoKWxwYFuz+JzFnbA5rQ08Yqjc7zi6BwAuwaG2bprgG27BtnaP8i2XYNsS4/jx7f0D/Bo385snl2DTPX3SWtTw7hQ2TdQJgucebOaaGqszybKEcGmHQM8tnknj27qZ13fTh7t2/u4bdfghMu1NjXQ2tTArObGsSAZe50/rSlNa9532mjwjE5rbWqgNX/+/OUdVFXPYWH7ZXZLI7NbZnPY/NnTWm5kJNi+Z2jSYBkNndHxx7fu4r4ntrFt1yA7B4anfO+5rU3MmzBY9g2chXNa6GhvYVFbCwtmN1dFyIyMBE9v3826vnFhkMIh/3vTIFiycDbLO9r4vRccxvKONubNbmZgaITdg8PsGRpJX8PsGcx/zJ8+zPbdQ9m0oRH2DI6wO823e2h4ysAvxmRBMhoyo9NbmrJwaU6PLU0NNDeKlsZGmps0NtbS2EDz2PQGWprSPI0aG2tNj2Pvlfd+1fBvoNKqJiwknQZcAjQCX4qIj1e4JNsPDXmHqKZrYGiEZ3bnB8vEQZOF0CC/fnoHW/sHeWbXIAPDIxO+p5R9Kn5RWwsdbS37BMne560smtPCovZsnlIdKhseCZ7YuotH+3Y+KxTW9fWzZ2jvNjQ3imUL53BkxxxOXLGI5R1zOLKzjeUdbSxZMJuWptL98osIhkZin+DZPZgXPONDaXCY3elxT/5jCqXdEwRWflANDo0wMDzCQHocHA6GR2b28HmDGAubfYNHtDQ10pIXOmOP+8zTsM/yrU0NtLc20dbaxNxZTbS3NtM+q4n21vQ1q4k5zY1V1c+tKsJCUiPwz8CrgQ3ALyStjoj7KluZlVNLUwOd7a10TvNS3ohg1+Bwdkhs5yBb+wfo2znA5gm+1vX1c8djW9nSPzDpL6Q5LY37hMqiOemxfXzgZCEzb/beE/4DQyNs2NLPur7+fUJhXV8/67f0Mzi8d52tTQ0s72jjyI42XnF0jiM72tLrORy+YHbFPkgpieZG0dzYwNyKVJAF6+Dw3hAZzHvcM5QFytj4BPNkwbN3nsG8MNp33kjvt3ds556hsbGB4REGhyI9puWHR4ra85KgvSULlNEgmZse21qf/XruuLDJf16ODtRVERbAiUBPRKwFkPRN4EzAYWEFSWJOSxNzWpqKPmw2MhJs3z1E3849+4RJ384BtuQ937xzgIee3sHmnQPsGpz4MFlTg1jY1kJLYwNPbttFfga1tTSyvLONYw+by2uOPzTbQ0ihcMjc1qr6y7OcGhtEY0PjQXlBREQWHjv3DLNj9xA79ox+DbJ991A2vmeQHbuH2L5niB27h9g5MMT2NO9T23bnLTNUVPC0NDbQ1tpI+6wmXrB0AZe+9YQZ365qCYslwPq81xuAl+TPIOl84HyAI444onyVWU1qaBDz5zQzf04zXbniltk1MMzm/gE27xigb+cetvQP0LcjC5Qt/QPsHhxh2cLZWRh0ZqHQ0dbik701RlI699LIorYDa+AZEfQPDO8NjxQoo8Gyc0/+60F27hnmsBLdYbNawqKgiLgMuAyyS2crXI7VodktjSxpmc2SBdM76W82GUm0pcNQiytcS7VcAvA4sCzv9dI0ZmZmZVAtYfELYKWkFZJagLOB1RWuycysblTFYaiIGJJ0IUQnG2sAAAbMSURBVPBDsktnr4iIeytclplZ3aiKsACIiGuAaypdh5lZPaqWw1BmZlZBDgszMyvIYWFmZgU5LMzMrKCavJ+FpF5g3TQX6wQ2laCcg1k9bjPU53bX4zZDfW73gWzzkRExYc+CmgyL/SFpzWQ3/ahV9bjNUJ/bXY/bDPW53aXaZh+GMjOzghwWZmZWkMNir8sqXUAF1OM2Q31udz1uM9Tndpdkm33OwszMCvKehZmZFeSwMDOzguoqLCSdJulBST2SLp5gequkq9L02yQtL3+VM6+I7b5I0n2S7pF0vaQjK1HnTCq0zXnz/b6kkFQTl1cWs92S3pR+3vdK+tdy1zjTivj3fYSkGyTdmf6Nn16JOmeSpCskbZT0q0mmS9Jn0vfkHkkHfp/ViKiLL7LW5g8DXUALcDdw3Lh5/hT4fHp+NnBVpesu03b/DjAnPX9XtW93Mduc5psL3AjcCqyqdN1l+lmvBO4EFqbXh1S67jJs82XAu9Lz44BHK133DGz3bwEnAL+aZPrpwH8CAk4CbjvQddbTnsWJQE9ErI2IAeCbwJnj5jkTuDI9/w5wiqr/BskFtzsiboiI/vTyVrI7EVazYn7WAH8LfALYXc7iSqiY7f5j4J8jYgtARGwsc40zrZhtDmBeej4feKKM9ZVERNwIbJ5iljOBr0TmVmCBpMMOZJ31FBZLgPV5rzeksQnniYghYBvQUZbqSqeY7c53HtlfJNWs4Dan3fJlEfGDchZWYsX8rI8GjpZ0s6RbJZ1WtupKo5ht/gjwNkkbyO6J8+7ylFZR0/1/X1DV3PzISk/S24BVwCsqXUspSWoA/hE4t8KlVEIT2aGo3ybbg7xR0vMiYmtFqyqttwBfjoh/kHQy8FVJx0fESKULqyb1tGfxOLAs7/XSNDbhPJKayHZZ+8pSXekUs91IehXwIeCMiNhTptpKpdA2zwWOB34i6VGyY7qra+AkdzE/6w3A6ogYjIhHgF+ThUe1KmabzwO+BRARtwCzyJrt1bKi/t9PRz2FxS+AlZJWSGohO4G9etw8q4Fz0vOzgB9HOltUxQput6QXAl8gC4pqP4YNBbY5IrZFRGdELI+I5WTnac6IiDWVKXfGFPNv/N/J9iqQ1El2WGptOYucYcVs82PAKQCSnkMWFr1lrbL8VgNvT1dFnQRsi4gnD+QN6+YwVEQMSboQ+CHZFRRXRMS9kv4GWBMRq4HLyXZRe8hOHp1duYpnRpHb/UmgHfh2Op//WEScUbGiD1CR21xzitzuHwKnSroPGAbeHxFVu/dc5Db/OfBFSe8jO9l9brX/ESjpG2Sh35nOxXwYaAaIiM+TnZs5HegB+oF3HPA6q/x7ZmZmZVBPh6HMzGw/OSzMzKwgh4WZmRXksDAzs4IcFmZmVpDDwiyPxKPS1B/YKmaeGaxnucSEnUXNyslhYWZmBTksrC5J/LvE7RL3Spw/wfTlEg9IfF3ifonvSMzJm+XdEndI/FLi2LTMiRK3SNwp8TOJYyZ4329K/G7e6y9LnJXW99/pPe+QeOkEy54rcWne66ul0U9jc2pa9x0S35ZoT+Mfl7hP4h6JTx3I98zqm8PC6tU7I3gRWePE90gTdhc+BvhsBM8BniG738moTRGcAHwO+Is09gDwmxG8EPjfwMcmeM+rgDcBSLSQtaH4AbAReHV6zzcDnyl2Q9Ihsb8GXpWWXwNclLbpDcBzI3g+8H+LfU+z8eqm3YfZOO+ReEN6voysmd74thfrI7g5Pf8a8B4Y++v8e+nxduB/pOfzgSslVpK1lWieYL3/CVwi0QqcBtwYwS6J+cClEr9B1obj6Glsy0lkN/W5Od19pQW4hazF/m7gcomrgaun8Z5m+3BYWN1Jh25eBZwcQb/ET8iay403vhdO/uvRzrzD7P1/9LfADRG8QWI58JNnvWGwO63vNWR7EN9Mk94HPA28gGyPf6IbMg2x79GA0ZoFXBfBW8YvIHEi2d7LWcCFwCsneF+zgnwYyurRfGBLCopjyf4yn8gREien528FbirifUfbQJ87xXxXkTV2+03g2rxln4xgBPhDsqZ44z0K/IZEg8QysrvEQdY192USRwFItEkcnc5bzI/gGrIwekGB+s0m5bCwenQt0CRxP/Bxsl+2E3kQuCDNt5Ds/MRU/h74O4k7mXqv/b/IbjD1owgG0thngXMk7gaOBXZOsNzNwCPAfWTnNO4AiKCXLJy+IXEP2SGoY8nu23F1GrsJuKhA/WaTctdZswmkw0hXR3B8pWsxOxh4z8LMzArynoWZmRXkPQszMyvIYWFmZgU5LMzMrCCHhZmZFeSwMDOzgv4/Hcf/AaTbCicAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myV8q0eOI2mL"
      },
      "source": [
        "**Comments :** \\\\\r\n",
        "**1.** It is obvious from the above graph that as the alpha value decreases from 1, the number of iterations increases gradually till alpha = 0.6, after that  iteration decreases to its minimum value 1 at alpha = 0.5 . \\\\\r\n",
        "**2.** Further decrease in alpha value leads to a rapid increase in the number of iterations. \\\\\r\n",
        "**3.** The minimizer remains same(i.e. [2, -3]) corresponding to every alpha value in the list [1, 0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01] whereas the function value starts from 0 (for alpha = 1 ), then increasing for decreasing alpha till alpha = 0.6, then suddenly falling to 0 at alpha = 0.5 . \\\r\n",
        "After that, again there is an increase in the function value. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wq7p2usOq0i",
        "outputId": "67f11b63-02f6-4636-e4e7-4d77c300c0b9"
      },
      "source": [
        "my_start_x = np.array([10,10])\r\n",
        "my_tol= 1e-9\r\n",
        "\r\n",
        "iterations_els = []\r\n",
        "\r\n",
        "x_opt_els, k, f_value = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\r\n",
        "print(\"Number of iterations in exact line search = \", k)\r\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of iterations in exact line search =  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6oJ8Oh5OHfX"
      },
      "source": [
        "**Comment 4 :** \\\\\r\n",
        "for **no** alpha value, gradient descent with backtracking line search takes lesser number of iterations when compared to the\r\n",
        "gradient descent procedure with exact line search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udy8VHuBREKN"
      },
      "source": [
        "**6 SOLUTION :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIVWoM1LRQmZ",
        "outputId": "a7a571fc-40bb-43a1-b14d-e6a019a7f0a1"
      },
      "source": [
        "my_start_x = np.array([10,10])\r\n",
        "my_tol= 1e-9\r\n",
        "rho_list = [0.9, 0.8, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01]\r\n",
        "iterat_bls = []\r\n",
        "\r\n",
        "for rho in rho_list:\r\n",
        "  x_opt_bls, k, f_value = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, rho, 0.5)\r\n",
        "  iterat_bls.append(k)\r\n",
        "  print('for rho = ',rho, ', iter:', k, ', x: ',x_opt_bls, 'f(x): ', f_value)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for rho =  0.9 , iter: 8 , x:  [ 2. -3.] f(x):  3.6998512426496585e-20\n",
            "for rho =  0.8 , iter: 15 , x:  [ 2. -3.] f(x):  1.2114568353050201e-20\n",
            "for rho =  0.75 , iter: 14 , x:  [ 2. -3.] f(x):  6.227537189331739e-21\n",
            "for rho =  0.6 , iter: 19 , x:  [ 2. -3.] f(x):  2.287501357495847e-19\n",
            "for rho =  0.5 , iter: 1 , x:  [ 2. -3.] f(x):  0.0\n",
            "for rho =  0.4 , iter: 16 , x:  [ 2. -3.] f(x):  1.0007255782441984e-20\n",
            "for rho =  0.25 , iter: 35 , x:  [ 2. -3.] f(x):  1.9735867671025198e-19\n",
            "for rho =  0.1 , iter: 109 , x:  [ 2. -3.] f(x):  1.7417011453747528e-19\n",
            "for rho =  0.01 , iter: 1195 , x:  [ 2. -3.] f(x):  2.498486159829666e-19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "2yEy7OlTTCdu",
        "outputId": "5e158bb4-110e-4a29-fafb-c01b8bda017b"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "#plt.figure(figsize=(width,height))\r\n",
        "plt.plot(rho_list, iterat_bls)\r\n",
        "plt.xlabel('rho values',color='b')\r\n",
        "plt.ylabel('Iterations',color='b')\r\n",
        "plt.title('Iterations against rho',color='r')\r\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFBP8D3xwt3Y"
      },
      "source": [
        "**Comments:** \\\\\r\n",
        "**1.** here for decreasing value of $rho$ the number of iterations first increases then decreases to 1 at $rho = 0.5$ . After that number of iterations starts increasing rapidly for rho $>$ 0.5 . \\\\\r\n",
        "**2.** for decreasing **rho** value first the function value decreases reaching value 0 at **rho** = 0.5 . Further decrease in **rho** value leads to an increase in the function value. \\\\\r\n",
        "**3.** for **no** rho value, gradient descent with backtracking line search takes lesser number of iterations when compared to the gradient descent procedure with exact line search"
      ]
    }
  ]
}