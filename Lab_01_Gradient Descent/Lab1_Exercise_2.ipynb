{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190011_IE684_Lab1_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1. Exercise 2. }$\r\n",
        "\r\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1-10)^2 + (x_2+2)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{[R]}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        "Write your answer here: \r\n",
        "\r\n",
        "**1.ANSWER:** \\\\\r\n",
        "$f(\\mathbf{x})=f(x_1,x_2)= (x_1-10)^2 + (x_2+2)^2$ \\\\\r\n",
        "$\\hspace{4cm}=\\ x_{1}^2 - 2*x_{1}*10 \\ +  \\ 10^2\\ + \\ x_{2}^2 \\ + \\ 2*2*x_{2} + \\ 4$ \\\\\r\n",
        "$\\hspace{4cm}=\\ (x_{1}^2 \\ + \\ x_{2}^2)\\ + \\ (- 2*x_{1}*10 \\   \\ + \\ 2*2*x_{2}+ \\ 10^2\\ + \\ 4)$ \\\\\r\n",
        "$  $ \\\\\r\n",
        "$\\hspace{4cm}= \\ \\begin{bmatrix}x_{1} & x_{2}\r\n",
        "\\end{bmatrix}\\ \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\x_{2} \\end{bmatrix} + 2\\begin{bmatrix}-10 & 2\\end{bmatrix}\\begin{bmatrix}\r\n",
        "x_{1} \\\\ x_{2}\\end{bmatrix} + 104$ \\\\\r\n",
        "$  $ \\\\\r\n",
        "$\\hspace{4cm}=\\ \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$  \\\\\r\n",
        "$ where ,\\ \\mathbf{x} = \\begin{bmatrix}x_{1} \\\\ x_{2}\r\n",
        "\\end{bmatrix}\\\\\r\n",
        "\\hspace{1.5cm} \\mathbf{A} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\ , \\ \\ \\mathbf{b} = \\begin{bmatrix}-10 \\\\ 2\\end{bmatrix}\\  \\ , \\ c = \\ 104 $\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqZygsmiLZZl"
      },
      "source": [
        "**2. QUESTION :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "\r\n",
        "$\\textbf{[R]}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jU-adJ0L-P1"
      },
      "source": [
        "**Write your answer here:** \\\\\r\n",
        "$ \\ Let ,\\\\\r\n",
        " g(\\alpha) = f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})) $\r\n",
        "\r\n",
        " $ \\  g^{'}(\\alpha) = (-\\nabla f(\\mathbf{x})^{T}) (f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$\r\n",
        " \r\n",
        "   $= (-\\nabla f(\\mathbf{x})^{T}) (2A(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})+2b)$\r\n",
        "\r\n",
        "   $= -2 [-\\nabla f(\\mathbf{x})^{T}AX-\\alpha\\nabla f(\\mathbf{x})^{T}A\\nabla f(\\mathbf{x})+\\nabla f(\\mathbf{x})^{T}b]$\r\n",
        "\r\n",
        "   $Now \\ for \\ extreme \\ values \\ (i.e. minima \\ or \\ maxima)$ \\\\\r\n",
        "   $ we \\ must \\ have \\ $\r\n",
        "    $ \\ g^{'}(\\alpha) = 0$\r\n",
        "\r\n",
        "\r\n",
        "$=> \\alpha\\nabla f(\\mathbf{x})^{T}A\\nabla f(\\mathbf{x}) = \\nabla f(\\mathbf{x})^{T}(Ax+b)$\r\n",
        "\r\n",
        "$=> \\alpha(\\nabla f(\\mathbf{x})^{T}A\\nabla f(\\mathbf{x})) = (\\nabla f(\\mathbf{x})^{T}\\nabla f(\\mathbf{x}))/2$\r\n",
        "\r\n",
        "$=> \\alpha = \\ (1/2) [(\\nabla f(\\mathbf{x})^{T}\\nabla f(\\mathbf{x})) / (\\nabla f(\\mathbf{x})^{T}A\\nabla f(\\mathbf{x}))]$ \\\\\r\n",
        "$ \\  \\ $ \\\\\r\n",
        "$This \\ is \\ the \\ required \\ solution.$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \r\n",
        "\r\n",
        "\r\n",
        "\\begin{align}\r\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\r\n",
        "& \\textbf{Initialize } k=0 \\\\ \r\n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \r\n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\r\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \r\n",
        "&\\quad \\quad k = {k+1} \\\\ \r\n",
        "&\\textbf{End While} \\\\\r\n",
        "&\\textbf{Output: } \\mathbf{x}^k\r\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/1.19/ for numpy documentation\r\n",
        "#we will first import the numpy package and name it as np\r\n",
        "import numpy as np \r\n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \r\n",
        "def evalf(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the objective function value\r\n",
        "  return (x[0]-10)**2 + (x[1]+2)**2\r\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \r\n",
        "def evalg(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the gradient value\r\n",
        "  return np.array([2*(x[0]-10),2*(x[1]+2)])"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength\r\n",
        "def compute_steplength(x): #add appropriate arguments to the function \r\n",
        "  #Complete the code \r\n",
        "  A = np.identity(2)\r\n",
        "  gr = evalg(x)\r\n",
        "  gr_t = np.matrix.transpose(gr)\r\n",
        "  step_length = np.matmul(gr_t, gr)/(2*np.matmul(np.matmul(gr_t, A), gr))\r\n",
        "  return step_length"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "  k = 0\r\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "    step_length = compute_steplength(x) #call the new function you wrote to compute the steplength\r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x) #compute gradient at new point\r\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  return x, k\r\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f242b5-75e8-477b-da55-b9c5b8fc3ee6"
      },
      "source": [
        "my_start_x = np.array([0,0])  \r\n",
        "tol_list = [10**(-j) for j in range(1,13)]\r\n",
        "iterations_list = []\r\n",
        "for my_tol in tol_list:\r\n",
        "  opt_x, k = find_minimizer(my_start_x, my_tol)\r\n",
        "  iterations_list.append(k)\r\n",
        "  print('for tolerance = ',my_tol,', iter:',k)\r\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for tolerance =  0.1 , iter: 1\n",
            "for tolerance =  0.01 , iter: 1\n",
            "for tolerance =  0.001 , iter: 1\n",
            "for tolerance =  0.0001 , iter: 1\n",
            "for tolerance =  1e-05 , iter: 1\n",
            "for tolerance =  1e-06 , iter: 1\n",
            "for tolerance =  1e-07 , iter: 1\n",
            "for tolerance =  1e-08 , iter: 1\n",
            "for tolerance =  1e-09 , iter: 1\n",
            "for tolerance =  1e-10 , iter: 1\n",
            "for tolerance =  1e-11 , iter: 1\n",
            "for tolerance =  1e-12 , iter: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "0WhbYxMOIQlF",
        "outputId": "87c7ca44-91b6-48e9-d93b-62bd9279a50c"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "#plt.figure(figsize=(width,height))\r\n",
        "plt.plot(tol_list, iterations_list)\r\n",
        "plt.xlabel('tolerance values',color='b')\r\n",
        "plt.ylabel('Iterations',color='b')\r\n",
        "plt.title('Iterations against Tolerance',color='r')\r\n",
        "plt.show()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjrDEwzQMEiL"
      },
      "source": [
        "**Comments :** \\\\\r\n",
        "**1.Here it is obvious from the above graph that the Iterations for each value of tolerance remains constant whereas  in the Exerxise 1 for constant  step length the number of Iterations  for increasing values of tolerance decreases.** \\\\\r\n",
        "**2.And this so because we are using exact line search for finding the minima.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYJaACtKMIGH"
      },
      "source": [
        "${\\Large\\text{Do not forget to rename the file before submission.}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb"
      },
      "source": [
        ""
      ],
      "execution_count": 93,
      "outputs": []
    }
  ]
}